{"title":"Thurmond PA - R Scripting","markdown":{"yaml":{"title":"Thurmond PA - R Scripting","author":[{"name":"Dan McGraw","affiliation":"USACE SAS EN-H","email":"daniel.e.mcgraw@usace.army.mil"}],"date":"2024-08-08","format":"html","code-overflow":"wrap","editor":"visual"},"headingText":"Overview","containsRefs":false,"markdown":"\n\n\nThis walkthrough is to provide description and explanation to R scripts used in the development of the J. Strom Thurmond Dam Hydrologic Hazard Curve (HHC). The readme file's contained in the project folder provide a summary of various scripts. Algorithm development is an iterative process so there is not one centralized script. The document will incorporate the contents of several scripts developed for the HHC. R is an open source, free software. All the libraries/functions used have extensive information online on their use and trouble shooting.\n\n## Period of Record Data Analysis\n\nData requests for existing period of record (POR) data should be completed through the home district’s water management section. Typical data stored for projects throughout the USACE portfolio range from observed lake elevation, stage, flow, precipitation, temperature, snow depth, snow water equivalent, and computed inflow at various time intervals.\n\nThe POR data for JST Dam was obtained from the (internal facing) Savannah District Water Management [website](https://esasw6kbaaa0101.sas.ds.usace.army.mil/). The data was copy-pasted from the historic data query into Notepad++ and saved as .txt file. The data needs to be delimited prior to use in R.\n\n### Data Import, Formatting, and Export\n\nThe following code blocks were taken from *ThurmondPOR_Data.R*. The first block below also serves as a format for scripting best practices. Commenting code is invaluable (comments are the only way I can remember what I did in the script, months later).\n\nThis section contains a commented header (note: *\\# Header \\#####* will name a section of code). It is good practice to clear the environmental at the start of a script. The necessary libraries are also called at the top of a script. In order to load a library, it has to have been previously downloaded. *install.packages* has been included and commented out as an example. The line *theme_USACE.r* is ggplot theme created by Brian [Breaker](\\share.hec.usace.army.mil\\breaker\\rTraining).\n\n```{r Code Heading and Libraries}\n#| warning: false\n# Thurmond POR data ############################################################\n# Inputs are raw text files copied from WM Site\n# Dan McGraw\n# 6-March-2024\n################################################################################\n\n## Clear workspace\nrm(list = ls(all.names = TRUE))\n\n## Load Libraries\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nsource(\"E:/R/theme_USACE.r\")\n```\n\nThe next step is to read the delimited test files containing the POR data. These are all saved in the same folder which is *pordir*. Note that the data should be in .csv or .txt files. R can read excel files, but it isn't as simple as read.csv/read.table\n\n```{r import data}\n# Read files ###################################################################\n# Locate directory/folder with POR data\npordir <- \"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/\"\n\n# Create file path of the POR txt files\nthur_elev_file <- paste0(pordir,\"ThurmondDL_3-6-2024_ELEV.txt\")\nthur_flows_file <- paste0(pordir,\"ThurmondDL_3-6-2024_NetInflow_Discharge.txt\")\nthur_stor_file <- paste0(pordir,\"ThurmondDL_3-6-2024_STOR.txt\")\nthur_gen_file <- paste0(pordir,\"ThurmondDL_3-6-2024_GEN.txt\")\n\n# Read text files\nthur_elev <- read.table(thur_elev_file)\nthur_flows <- read.table(thur_flows_file)\nthur_stor <- read.table(thur_stor_file)\nthur_gen <- read.table(thur_gen_file)\n\n```\n\nHeadings were not read in with the data. These were added after import based on existing user knowledge of the fields. This code block is using *dplyr* from the tidyverse package. Tidyverse works very well with the pipe operator (%\\>%). Think of this as:\n\n*new data name* \\<- *old data name* \\>input\\> *function*\n\n```{r rename cols}\nthur_elev <- thur_elev %>% rename(Date = V1,DOW = V2, Elev_29 = V3) \nthur_flows <- thur_flows %>% rename(Date = V1,DOW = V2, NetInflow = V3, Discharge = V4) \nthur_stor <- thur_stor %>% rename(Date = V1,DOW = V2, Storage = V3) \nthur_gen <- thur_gen %>% rename(Date = V1,DOW = V2, Gen = V3)\n```\n\nAn example of the *thur_flows* dataset is below. Note the data was downloaded in descending date, this will be important later.\n\n```{r head}\nhead(thur_flows)\n```\n\nThe day of the week was not significant for this analysis. It was dropped from the dataset.\n\n```{r}\nthur_elev <- thur_elev %>% select (-c(DOW))\nthur_flows <- thur_flows %>% select (-c(DOW))\nthur_stor <- thur_stor %>% select (-c(DOW))\n```\n\nThe date field will be very important for the analysis. R has read the date field as a character data-type. This means there is no numerical basis to the values (the dates are just words). The dates need to be converted a date data-type for this analysis. For example, once the dates are formatted correctly, they can be arranged by increasing date. This is done using the *as.Date* function. Errors can arise when the dates are not formatted uniformly. The function allows several \"try\" inputs for cases like that. Another breakdown of the pipe operator is shown above the code block:\n\n*thur_flows_updated* = *thur_flows_existing* + a new DATE column (*mutate*), called *DT*, that will go before *NetInflow*\n\n```{r dates}\nthur_flows <- thur_flows %>% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = NetInflow )\nthur_elev <- thur_elev %>% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Elev_29)\nthur_stor <- thur_stor %>% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Storage)\n```\n\nSee the difference:\n\n```{r test}\nclass(thur_flows$Date)\nclass(thur_flows$DT)\n```\n\nNow the dates can be arranged by increasing date.\n\n```{r dates incr}\nthur_elev <- thur_elev %>% arrange(DT)\nthur_flows <- thur_flows %>% arrange(DT)\nthur_stor <- thur_stor %>% arrange(DT)\n```\n\nAn advantage to using R instead of excel is the ability to do column-wide functions without click & drag. The elevation data needs to be in NAVD88. The conversion from NGVD29 to NAVD88 is: $$ NAVD88 = NGVD29 - 0.7$$\n\n```{r elev88}\n# Create a new dataframe (df) for NAVD88 elevation\nthur_elev88 <- thur_elev %>% mutate(Elev_88 = Elev_29 - 0.7)\n\n# Drop the ngvd29 column from the navd88 df to avoid confusion\nthur_elev88 <- thur_elev88 %>% select(-c(Elev_29))\n```\n\nAt this point of the analysis the data was exported as csv files. These csv files could serve as a starting point for analysis in different scripts, without having to clean up the data as much (date will always need to be formatted *as.Date*). Note that an R & DSS package exists: <https://github.com/eheisman/dssrip>. The *write.csv* function has been commented out so the code will not export any files during this walkthrough.\n\n```{r export}\nthur_elev_outfile <- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV.csv\")\nthur_elev88_outfile <- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV_88.csv\")\nthur_flows_outfile <- paste0(pordir,\"csv/Thurmond_3-6-2024_NetInflow_Discharge.csv\")\nthur_stor_outfile <- paste0(pordir,\"csv/Thurmond_3-6-2024_STOR.csv\")\n\n#write.csv(thur_elev,thur_elev_outfile)\n#write.csv(thur_elev88,thur_elev88_outfile)\n#write.csv(thur_flows,thur_flows_outfile)\n#write.csv(thur_stor,thur_stor_outfile)\n```\n\n### Historic Inflows & Pool Elevations\n\nAn important first step of data analysis is understanding what the data looks like. R was used to quickly provide historic pool levels, large inflows, and additional time series context.\n\n```{r}\n#| warning: false\n# What years are contained within the POR\nthryears <- unique(year(thur_flows$DT))\n\n# What does the POR data look like\nggplot(data = thur_flows,aes(x=DT,y=Discharge)) + geom_point()+theme_USACE()\n```\n\nHow many inflows above 80,000 cfs (arbitrary) have there been?\n\n```{r}\n#| warning: false\nthur_flows %>% filter(NetInflow > 80000)%>%\n  ggplot(aes(x=DT,y=NetInflow))+geom_point() + theme_USACE()\n```\n\nThe top 10 largest pool elevations and inflows can be obtained as follows:\n\n```{r}\n# MAX Pool Elevations (top 10)\nthur_elev %>% top_n(10,Elev_29)%>% arrange(desc(Elev_29))\n```\n\n```{r}\n# MAX Pool Elevations (top 10)\nthur_flows %>% top_n(10,NetInflow) %>% arrange(desc(NetInflow))\n```\n\n### Creating Annual Maxima Data\n\nHydrologic analysis uses annual maxima series (AMS) based on water years (starting on October 1). Using the correctly formatted date column, an AMS can be created for inflow and reservoir pool elevation/stage. Note that the reservoir inflow is regulated. The first step is determining the water year of each data point.\n\n```{r water year}\n# Create separate columns of Year, Month, Day as numbers\nthur_flows <- thur_flows %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\n\n# ifelse is a binary yes/no operator. If the month greater than 10, WY = Yr+1\nthur_flows <- thur_flows %>% mutate(WaterYear = ifelse(Mon >= 10, Yr + 1, Yr))\n\nthur_elev88 <- thur_elev88 %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\nthur_elev88 <- thur_elev88 %>% mutate(WaterYear = ifelse(Mon >= 10, Yr + 1, Yr))\n\n```\n\nNow that the data can be grouped by water year, the annual max of each water year can be obtained\n\n```{r ams}\nannual_max_flows <- thur_flows %>%\n  group_by(WaterYear) %>%\n  summarize(MaxInflow = max(NetInflow),\n            Date = DT[which.max(NetInflow)],\n            Month = month(DT[which.max(NetInflow)]),\n            Day = day(DT[which.max(NetInflow)]))\n\nannual_max_stage88 <- thur_elev88 %>%\n  group_by(WaterYear) %>%\n  summarize(Max_Elev = max(Elev_88),\n            Date = DT[which.max(Elev_88)],\n            Month = month(DT[which.max(Elev_88)]),\n            Day = day(DT[which.max(Elev_88)]))\n\n# export if necessary\n# write.csv(annual_max_stage88,paste0(pordir,\"Stage_AMS.csv\"))\n# write.csv(annual_max_flows,paste0(pordir,\"Reg_Flow_AMS.csv\"))\n```\n\n### Critical Inflow Duration Analysis\n\nThis section of code aimed to determine the distribution of inflow durations at JST Dam. The procedure used was from from RMC-TR-2018-03 - \"Hydrologic Hazard Methodology for Semi-Quantitative Risk Assessments\". The TR defines *critical inflow duration* as the inflow duration that results in the highest water surface elevations for the reservoir of interest.\n\nAccording to RMC-TR-2018-03, three to 5 (3-5) historical peak reservoir events should be identified. The inflow, discharge, and stage for these events should be used to determine the critical inflow duration. The TR states \"select events that are consistent with the types of events likely to be the driver of extreme peak stages\".\n\nDue to the highly regulated nature of JST dam, the driver of extreme stage could vary from shorter, localized inflows to the dam OR longer, basin-wide inflows to the dam. In order to reach an understanding of basin operations as well as a good starting point for the critical inflow duration, the following analyses were done:\n\n#### Analyze the reservoir elevation rate of change\n\n1.  Determine the daily rate of change of reservoir elevation (ft/day)\n2.  Identify the top 150 daily rates (increasing)\n\n```{r wse rate}\n# Create and Set WSE Rate column to 0\nthur_elev88$WSERate <- NA\nthur_elev88$WSERate[1]<-0\n\n# Get rate of changer per day\nfor (i in 2:length(thur_elev88$Elev_88)){\n  day2 = thur_elev88$Elev_88[i]\n  day1 = thur_elev88$Elev_88[i-1]\n  stagedelta = day2 - day1\n  thur_elev88$WSERate[i] = stagedelta\n}\n\n# Select the top 150 rates\nFastRise <- thur_elev88 %>%\n  arrange(desc(WSERate)) %>% slice_max(WSERate,n=150)\n\n# Plot the rates against their corresponding reservoir elevations\nggplot(data=FastRise,aes(x=WSERate,y=Elev_88))+geom_point()+theme_USACE()+\n  labs(x = \"Max Daily Rate of Change (ft/day)\", y = \"Peak Reservoir Elev (NAVD88)\")\n\n```\n\nThis is helpful, but it can only tell so much since the inflow data is daily.\n\n#### Determine the inflow duration corresponding to the reservoir pool AMS\n\n1.  Loop through all high pool events and obtain corresponding inflow and discharge hydrographs from 7 days before and after\n2.  Find days during event where inflow \\> discharge\n3.  Save the hydrograph image and table\n4.  Create distribution of inflow durations\n\nThis code will save a csv and png of the inflow event\n\n```{r crit inflow}\n# NOTE annual_max_stage88$Date is a formatted date column\nams_dates <- annual_max_stage88$Date\n# ams_dates <- annual_max_flows$Date\n\neventDTs <- ams_dates\n\n# Grab flow hydrographs 14 days before and after max pool\neventDTs_start <- eventDTs - 7\neventDTs_end <- eventDTs + 7\n\nthur_rep_hydrographs <- list()\nthur_flood_durations <- list()\nflow.colors <- c(\"Inflow\" = \"#333BFF\", \"Outflow\" = \"orangered2\")\n\nfor (i in 1:length(eventDTs)){\n  hist_pool_date <- eventDTs[i]\n  \n  dt1 <- which(thur_flows$DT == eventDTs_start[i])\n  dt2 <- which(thur_flows$DT == eventDTs_end[i])\n  \n  if (eventDTs_start[i] < thur_flows$DT[1]){\n    dt1 <- 1\n  }\n  \n  hydrograph <- thur_flows[dt1:dt2,]\n  hydrograph$Event <- eventDTs[i]\n  thur_rep_hydrographs[[i]] <- hydrograph\n  \n  plot_date <- as.character(format(hist_pool_date, \"%d-%b-%Y\"))\n  flood_dur_plot<-ggplot(data = hydrograph)+\n    geom_line(aes(x=DT,y=NetInflow,color=\"Inflow\"))+geom_point(aes(x=DT,y=NetInflow,color=\"Inflow\"))+\n    geom_line(aes(x=DT,y=Discharge,color=\"Outflow\"))+geom_point(aes(x=DT,y=Discharge,color=\"Outflow\"))+\n    theme_USACE()+\n    ggtitle(paste0(\"Record Pool Event on \",plot_date))+\n    scale_x_date(date_breaks = \"1 day\")+labs(x=\"Date\",y=\"Discharge(cfs)\",color = \"Legend\")+\n    scale_color_manual(values = flow.colors)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n    theme(plot.title = element_text(hjust = 0.5))+\n    theme(legend.position = c(0.8,0.8))\n  \n  # Find where Inflow > outflow for flood duration\n  flood_dates <- hydrograph$DT[hydrograph$NetInflow >= hydrograph$Discharge]\n  peakflowdate <- hydrograph$DT[max(hydrograph$NetInflow) == hydrograph$NetInflow]\n  event_index <- which(flood_dates == peakflowdate)\n  \n  if (length(event_index) == 0){\n    event_index <- length(flood_dates)+3\n  }\n  \n  # Find consecutive dates around eventDT\n  start_index <- event_index\n  end_index <- event_index\n  \n  # Expand the start index to include previous consecutive dates\n  while (start_index > 1 && flood_dates[start_index] - flood_dates[start_index - 1] == 1) {\n    start_index <- start_index - 1\n  }\n  \n  # Expand the end index to include following consecutive dates\n  while (end_index < length(flood_dates) && flood_dates[end_index + 1] - flood_dates[end_index] == 1) {\n    end_index <- end_index + 1\n  }\n  \n  # Extract consecutive dates\n  consecutive_dates <- flood_dates[start_index:end_index]\n  \n  # Flood Duration in Days\n  flood_dur <- length(consecutive_dates)\n  \n  # march 21 2006 is tough, duration likely = 4-5 but outflow was always larger than inflow\n  # peak outflow matched peak inflow\n  \n  thur_flood_durations[[i]] <- data.frame(Event = hist_pool_date,Dur_days = flood_dur)\n\n  # Write CSV - uncomment to save\n  #write.csv(hydrograph,paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Flood_\",hist_pool_date,\".csv\"),row.names = F)\n  \n  # Save plot of flood event - uncomment to save \n  #ggsave(paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Plots/\",hist_pool_date,\".png\"),\n         #flood_dur_plot,width = 8, height = 6,dpi = 300)\n}\n\n# Convert the results to a usable df\nthur_dur <- do.call(rbind.data.frame,thur_flood_durations)\n```\n\n*thur_dur* contains the inflow duration of each high pool event. This was plotted as a histogram with a binwidth of 1-day to determine an initial estimate for the critical inflow duration.\n\n```{r inflow stats}\nmdur <- mean(thur_dur$Dur_days)\nsddur <- sd(thur_dur$Dur_days)\nmeddur <- median(thur_dur$Dur_days)\n\n# Base R does not have a Mode function.\nMode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Compute the mode\nprint(paste0(\"The mode is: \",Mode(thur_dur$Dur_days),\" days\"))\n```\n\n```{r plot colors}\n#| echo: false\ncolorpal <- brewer.pal(9,\"Set1\")\nhistcolor <- colorpal[2]\nlinecolor<- \"white\"#colorpal[9]\nmeancolor <- colorpal[1]\n#medcolor <- colorpal[3]\nsdcolor <- colorpal[3]\n```\n\nThe resulting hyetograph is shown below.\n\n```{r duration hist}\n#| warning: false\n\n# setting up a ggplot looks worse than it is\nggplot(data = thur_dur,aes(x=Dur_days))+\n  geom_histogram(binwidth=1,fill=histcolor,color=linecolor,alpha=0.9)+\n  scale_x_continuous(breaks = seq(1,14,1))+\n  theme_USACE()+\n  labs(x = \"Flood Duration (Days)\",y=\"Count\",\n       subtitle = \"Based on AMS Flood Events (1962 - 2024)\")+\n  ggtitle(\"Distribution of Flood Durations at Thurmond Dam\")+\n  geom_vline(xintercept = mdur, color=meancolor)+\n  geom_vline(xintercept = (mdur+sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_vline(xintercept = (mdur-sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_text(aes(x=mdur, label=paste(\"\\nMean = \",round(mdur,2),sep=\"\"),\n                y=7.5), colour=meancolor, angle=90)+\n  geom_text(aes(x=(mdur+sddur), label=paste(\"\\n+1SD = \",round(mdur+sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  geom_text(aes(x=(mdur-sddur), label=paste(\"\\n-1SD = \",round(mdur-sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n```\n\nBased on the results of the AMS duration distribution, the critical inflow duration is likely 3-, 4-, or 5-days. With this in mind, five historical peak reservoir events were identified to estimate a final critical duration. The loop used to create the inflow duration distribution contained a code to export a csv and png of each AMS event. These were used to estimate a final critical inflow duration. This has been replicated below.\n\n```{r final crit duration}\nevent <- c(\"11-Apr-1964\",\"30-Mar-1964\",\"3-Mar-1971\",\"17-Mar-1975\",\"7-Feb-1998\",\"1-Jan-2016\")\nduration <- c(5,4,4,4.5,4.5,4)\n\nmean_inflow_dur <- round(mean(duration),2)\ngeomean_inflow_dur <- round(exp(mean(log(duration))),2)\n\ncat(\"The mean inflow duration is \",mean_inflow_dur,\"\\nThe geometric mean inflow duration is \", geomean_inflow_dur)\n\n```\n\nBoth 4-day and 5-day could be considered for the critical inflow duration. During the project, the 4-day inflow was selected as the critical duration.\n\nAs a preemptive analysis of seasonallity, the stage seasonallity was analyzed.\n\n```{r stage season}\n#| warning: false\nggplot(data = annual_max_stage88,aes(x = Month))+\n  geom_histogram(binwidth=1,fill=\"lightblue\",color=\"black\",alpha=0.9)+\n  stat_density(aes(y = ..count..), geom = \"line\", color = \"green3\", size = 1) +\n  scale_x_continuous(breaks = seq(1,12,1),\n                     labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"June\",\"July\",\"Aug\",\"Sept\",\"Oct\",\"Nov\",\"Dec\"))+\n  scale_y_continuous(breaks = seq(0,20,2),minor_breaks = seq(0,20,1))+\n  theme_USACE()+\n  labs(x = \"Month\",y=\"Count\",subtitle = \"Based on AMS of POR Stage Elev.(1962 - 2024)\")+\n  ggtitle(\"Flood Seasonality at Thurmond Dam\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))\n```\n\n## Unregulated 4-day Inflow AMS\n\nThe next steps were to create a timeseries of unregulated inflow data that can be used to create an AMS of 4-day inflow volumes. This section incorporates code used in *CreatingUnregulatedData_Augusta.R* and *Crtical Volume Duration.R*.\n\nThe Augusta USGS Streamgage was the subject of a 1990 USACE & USGS report that analyzed the relationship between regulated and unregulated flow-frequency estimates. Due to the existence of this study and the longer gage record, the Augusta gage was used as the primary data source for inflow data. The overlapping record of the Augusta gage with the former USGS streamgage at JST Dam was used to verify the drainage area ratio from the design memo (0.93).\n\n$$ Q_T = 0.93 * Q_A$$\n\nIn order to create a 4-day inflow volume annual maximum series (AMS), the daily streamflow data at the Augusta gage was processed using the following steps:\n\n1.  The effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\n\n2.  Daily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in Figure 14 (PA Report - Chapter 4). This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n\n3.  4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n\n4.  4-day unregulated volume AMS was obtained for each water year\n\n5.  The peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\n\n6.  The peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\n\n7.  The drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\nThese steps & figures will be replicated below.\n\n### Unregulated Inflow Data\n\nBefore any of the steps above, the data should be imported and formatted.\n\n```{r}\n#| warning: false\n# Import Augusta Daily & Format Dates\nlibrary(\"dataRetrieval\")\n\n# Import using Augusta Site Number\nsiteNo <- \"02197000\"\npCode <- \"00060\" #00061 - # Check codes from USGS\nstart.date <- \"1700-01-01\" # arbitraty to get all info\nend.date <- \"2024-12-31\"\n\naugusta_daily <- readNWISdv(siteNumbers = siteNo,\n                            parameterCd = pCode,\n                            startDate = start.date,\n                            endDate = end.date)\n\naugusta_daily <- renameNWISColumns(augusta_daily)\n\naugusta_daily <- augusta_daily %>% mutate(DT = as.Date(Date),.before = Flow)\n\n# Add water year column\naugusta_daily <- augusta_daily %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_daily <- augusta_daily %>% mutate(WaterYear = ifelse(Mon > 9, Yr + 1, Yr))\n\n# Make sure the data is there\nggplot(data = augusta_daily) + geom_line(aes(x=DT,y=Flow)) + theme_bw()\n\n```\n\nThe regulated-unregulated relationship was digitized from the 1990 report and imported into R.\n\n```{r unreg-reg}\n# Import Digitized Data for Figure 35 (1990) - Exceedance Prob for Unregulated and Regulated\nfig35 <- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Figure35data.csv\")\nfig35 <- fig35 %>% mutate(NEP = 100-ExceedanceProb)\n\n# Convert Reg and Unreg at Augusta\naugusta_conversion <- fig35 %>% mutate(Aug_RegQ = PeakQRegulated,Aug_UnregQ = PeakQUnregulated)\naugusta_conversion <- augusta_conversion %>% select(c(Aug_RegQ,Aug_UnregQ))\n\n# Recreate Figure 35\nlogscalebreaks = c(1000,5000,10000,20000,50000,100000,200000,500000,1000000)\nlogscaleminor_breaks = c(seq(2000, 9000, by = 1000),seq(20000, 90000, by = 10000),\n                         seq(200000, 900000, by = 100000))\n\nggplot(data=fig35,aes(x=PeakQUnregulated,y=PeakQRegulated))+\n  geom_point()+\n  geom_line()+\n  theme_USACE()+\n  ggtitle(\"Savannah River at Augusta, GA (02197000)\")+\n  labs(x=\"Unregulated Discharge (cfs)\",y=\"Regulated Discharge (cfs)\",subtitle = \"Relation between regulated peak discharges and unregulated peak discharges\",\n       color = \"Legend\")+\n  theme(plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5),legend.position = \"bottom\")+\n  geom_abline(intercept = 0, slope = 1, color = \"blue1\", linetype = \"dashed\") +\n  scale_y_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  scale_x_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  coord_cartesian(xlim = c(5000,1000000),ylim = c(5000,1000000))\n```\n\nThe Peak Flow dataset at Augusta will also be used. This data was obtained from the USGS website. Additional post processing was done to make this easier (i.e. some manual entry to Flow Notes)\n\n```{r peak flow}\n# Import and Format\naugusta_peak_ams <- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Augusta Gage/csv/Augusta_Peak_AMS.csv\",header = T)\naugusta_peak_ams <- augusta_peak_ams %>% mutate(DT = as.Date(Date),.before = Flow)\naugusta_peak_ams <- augusta_peak_ams %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_peak_ams <- augusta_peak_ams %>% mutate(WaterYear = ifelse(Mon > 9, Yr + 1, Yr))\naugusta_peak_ams <- augusta_peak_ams %>% rename(Peak_Flow = Flow)\n```\n\n1.  The effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\n\n```{r start of reg}\n# Regulation Assumed to start in 1951\nregulation_wy <- 1951\npre_reg <- augusta_daily %>% filter(WaterYear < regulation_wy)\naug_reg <- augusta_daily %>% filter(WaterYear >= regulation_wy)\n\npre_reg_peak <- augusta_peak_ams %>% filter(WaterYear < regulation_wy)\naug_reg_peak <- augusta_peak_ams %>% filter(WaterYear >= regulation_wy)\n```\n\n2.  Daily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in the relationship above. This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n\n```{r}\n# Estimate Unregulated from approx function\nAugusta_Unreg <- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ, xout = aug_reg$Flow)[2]\nAugusta_Unreg_peak <- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ,\n                             xout=aug_reg_peak$Peak_Flow)[2]\n\n# add to regulated data\naug_reg <- aug_reg %>% mutate(Unregulated_Flow = round(Augusta_Unreg$y,0))\naug_reg_peak <- aug_reg_peak %>% mutate(Unregulated_PeakFlow = round(Augusta_Unreg_peak$y,0))\n\n# create a 1:1 column of unregulated flow to make bind_rows easier\npre_reg <- pre_reg %>% mutate(Unregulated_Flow = round(Flow,0))\npre_reg_peak <- pre_reg_peak %>% mutate(Unregulated_PeakFlow = round(Peak_Flow,0))\n\n# Combine datasets\naugusta_daily_unreg <- bind_rows(pre_reg,aug_reg)\nAMS_Peak <- bind_rows(pre_reg_peak,aug_reg_peak)\n\n# remove any confusion\nrm(Augusta_Unreg)\nrm(Augusta_Unreg_peak)\n\n# Check results for sanity \nggplot(data = augusta_daily_unreg) +\n  geom_line(aes(x=DT,y=Unregulated_Flow,color = \"Unregulated\"))+\n  geom_line(aes(x=DT,y=Flow,color=\"Existing\"))+\n  theme_USACE()+\n  labs(x = \"Date\",\n       y = \"Daily Flow (cfs)\",\n       title = \"Daily Flow Values Augusta (02197000)\",\n       subtitle = \"Post-WY 1951 Converted to Unregulated\")+\n  scale_y_continuous(breaks = seq(0,250000,50000),minor_breaks = seq(0,250000,10000))+\n  scale_x_date(breaks = \"10 year\",minor_breaks = \"1 year\")+\n  scale_color_manual(values = c(\"#377EB8\", \"red2\"), \n                     name = \"Legend\", \n                     labels = c(\"Existing\",\"Unregulated\"))+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),legend.position = c(.85,.8))\n```\n\n3.  4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n\n```{r 4 day vols}\n# Create Data with gaps to avoid rolling mean over the gaps (i.e. 1891-12-31 & 1896-01-01)\ncomplete_dates <- seq(min(augusta_daily_unreg$DT), max(augusta_daily_unreg$Date), by = \"day\")\naug_unreg_complete <- merge(augusta_daily_unreg, data.frame(Date = complete_dates), by = \"Date\", all = TRUE)\n# aug_unreg_complete[3010:3020,]  # check\n#naniar::vis_miss(aug_unreg_complete) # see gaps in data, uncomment to use\n\ncritdur <- 4\n# rollmean is a 4 day rolling average. rollmeanr indicates that the window is aligned to the \"right\"\n# This will compute a time series of 4-day inflow volumes\naug_unreg_complete <- aug_unreg_complete %>% mutate(VolDur_4day = round(rollmeanr(Unregulated_Flow,k = critdur, fill=NA),0))\n\n```\n\n4.  4-day unregulated volume AMS was obtained for each water year\n\n```{r 4 day ams}\nAMS_4day <- aug_unreg_complete %>%\n  # Group by WY\n  group_by(WaterYear) %>%\n  # Take the largest value from each WY\n  top_n(1, VolDur_4day) %>%\n  # Ensures no duplicate years\n  distinct(WaterYear, .keep_all = TRUE) %>%\n  ungroup()\n\n# Combine Peak and 4-day AMS to help stay organized\nall_AMS <- left_join(AMS_Peak,AMS_4day,by = \"WaterYear\")\ncolnames(all_AMS)\nall_AMS <- all_AMS %>% select(c(DT.x,Peak_Flow,Flow_notes,WaterYear,Unregulated_PeakFlow,DT.y,Unregulated_Flow,VolDur_4day))\nall_AMS <- all_AMS %>% rename(DT_Peak = DT.x, DT_Daily = DT.y)\n```\n\n5.  The peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\n\n```{r peak to vol}\n# Find the peaks and 4-day vols that are from the same event. 4 is the search threshold then\nmatchingAMS <- all_AMS %>% filter(DT_Peak >= DT_Daily - 4 | DT_Peak <= DT_Daily + 4)\nmatchingAMS <- matchingAMS %>% mutate(PeaktoVol = Unregulated_PeakFlow/VolDur_4day)\n\n# Now Add this peak to vol data back to the AMS dataset\nall_AMS <- all_AMS %>% mutate(Ratio_Peak_4d = ifelse(DT_Peak >= DT_Daily - 4 | DT_Peak <= DT_Daily + 4,Unregulated_PeakFlow/VolDur_4day,NA))\n\ncat(\"The mean peak-to-volume ratio is \",round(mean(matchingAMS$PeaktoVol),2),\"\\nThe geometric mean peak-to-volume ratio is \", round(exp(mean(log(matchingAMS$PeaktoVol))),2),\"\\nThe median peak-to-volume ratio is \",round(median(matchingAMS$PeaktoVol),2))\n```\n\n```{r peak to vol plot}\nggplot(data = matchingAMS)+\n  geom_histogram(aes(x =PeaktoVol),fill=\"lightblue\",color=\"black\",alpha=0.9,\n                 breaks = seq(0.9,3.0,0.1))+\n  theme_USACE()+\n  ggtitle(\"Ratio of Peak Flow to 4-Day Volume at Augusta (02197000)\")+\n  labs(x=\"Peak Flow/4-Day Vol\", y=\"Count\")+\n  scale_x_continuous(breaks = seq(0.9,3.0,0.1),minor_breaks = seq(0,3.0,0.05))+\n  scale_y_continuous(breaks = seq(0,25,1))+\n  geom_vline(xintercept = mean(matchingAMS$PeaktoVol), color=\"orange2\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+\n  theme(axis.text.x = element_text(size=6.5))\n```\n\n6.  The peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\n\n```{r pvt missing vals}\n# Peak to volume ratio\nPtVratio_4day <- mean(matchingAMS$PeaktoVol)\n\n# Calculate missing 4 day volumes\nall_AMS <- all_AMS %>% mutate(Peak_to_4day = round(Unregulated_PeakFlow/PtVratio_4day,0))\n\nall_AMS <- all_AMS %>% mutate(Complete_4Day_AMS = ifelse(is.na(all_AMS$VolDur_4day),Peak_to_4day,VolDur_4day))\n```\n\n7.  The drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\n```{r Thurmond DA ratio}\n# Previously calculated drainage area ratio\nDAratio <- 0.93\n\n# Multiply the Augusta AMS values by the DA ratio\nall_AMS <- all_AMS %>% mutate(Thurm_AMS_4day = round(DAratio*Complete_4Day_AMS,0))\n```\n\nBONUS STEP: Prepare the data to be used in BestFit. This will use the flow notes to set perception threshold values & interval values (assumed to be +/- 20%). The interval calculation can be tough to decipher via code. It has been written out as an equation below.\n\n$$ Interval_L = 0.80 * 0.93 * PeakQ/PeakToVol$$ $$ Interval_U = 1.20 * 0.93 * PeakQ/PeakToVol$$\n\n```{r Best Fit Prep}\n# Rename some columns to make it easier - this is because my variable naming was not as intuitive as it should have been\nall_AMS <- all_AMS %>% rename(Aug_AMS_1day = Unregulated_Flow, \n                              Aug_AMS_4day = VolDur_4day,\n                              Aug_Complete_4Day_AMS = Complete_4Day_AMS)\n\n# If there is no peak to 4day ratio, this was historic peak flow information\nall_AMS <- all_AMS %>% mutate(BestFitNotes = ifelse(is.na(Ratio_Peak_4d),\"Estimated from Peak AMS\",NA))\n\n# Computing intervals. ifelse ensures it only computs missing ones\nall_AMS <- all_AMS %>% mutate(Interval_Lower =\n                                ifelse(WaterYear<1876,round((DAratio*0.8*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_MostLikely =                                 \n                                ifelse(WaterYear<1876,round((DAratio*1.0*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_Upper =\n                                ifelse(WaterYear<1876,round((DAratio*1.2*Unregulated_PeakFlow)/PtVratio_4day,0),NA))\n```\n\nThe *allAMS* dataframe can be exported to .csv or it can be further filtered to only include fields required for BestFit.\n\n```{r export bestfit}\n# Exporting allAMS - commented out\n# write.csv(all_AMS,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_AMS_Peak_1day_4day.csv\",row.names = F)\n\n# Selecting fields from allAMS that are required for BestFit\nbestfit_input <- all_AMS %>% select(c(WaterYear,Peak_Flow,Flow_notes,Unregulated_PeakFlow,\n                                      Thurm_AMS_4day,BestFitNotes,\n                                      Interval_Lower,Interval_MostLikely,Interval_Upper))\n\n# Export BestFit to csv - commented out\n#write.csv(bestfit_input,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_BestFitINPUT.csv\",row.names = F)\n\n```\n\n## Theme USACE\n\nMove this before the other code blocks for ggplot themes. If issues continue, remove *+theme_USACE()* from the ggplot chunks\n\n```{r theme USACE}\ntheme_USACE <-  function(base_size = 8){theme(\n  text = element_text(family = 'serif', color = 'black'),\n  line = element_line(colour = 'black', linewidth = 0.2), \n  rect = element_rect(colour = 'black', linewidth = 0.2),\n  plot.title = element_text(vjust = 3, size = 9),\n  plot.margin = unit(c(1,1,1,1), 'lines'),\n  panel.border = element_rect(fill = F),\n  panel.grid.major = element_line(colour = 'grey50', linewidth = 0.2),\n  panel.grid.minor = element_line(colour = 'grey75', linewidth = 0.1),\n  panel.background = element_rect(fill = 'white'),\n  #defaults legend to upper left, can/should be overridden based on graph\n  #legend.background = element_blank(),\n  legend.background = element_rect(fill = \"lightgrey\", colour = \"black\"),\n  legend.justification = c(\"left\", \"top\"),\n  legend.position = c(0.8, 0.5),\n  # this value should be adjusted dependent on \n  # graph with the addition of another \n  # theme(legend.position = c(X, Y)) argument after theme_USACE()\n  # or... for no legend \n  # legend.position = element_blank(),\n  legend.key = element_blank(),\n  legend.title = element_text(size = 9),\n  #legend.title = element_blank(), \n  axis.title.x = element_text(size = 9),\n  axis.title.y = element_text(angle = 90, size = 9),\n  axis.text.x = element_text(margin = margin(8, 0, 0, 0)),\n  axis.text.y = element_text(margin = margin(0, 8, 0, 0)),\n  axis.ticks.length = unit(0.25 , 'cm')\n)}\n```\n","srcMarkdownNoYaml":"\n\n# Overview\n\nThis walkthrough is to provide description and explanation to R scripts used in the development of the J. Strom Thurmond Dam Hydrologic Hazard Curve (HHC). The readme file's contained in the project folder provide a summary of various scripts. Algorithm development is an iterative process so there is not one centralized script. The document will incorporate the contents of several scripts developed for the HHC. R is an open source, free software. All the libraries/functions used have extensive information online on their use and trouble shooting.\n\n## Period of Record Data Analysis\n\nData requests for existing period of record (POR) data should be completed through the home district’s water management section. Typical data stored for projects throughout the USACE portfolio range from observed lake elevation, stage, flow, precipitation, temperature, snow depth, snow water equivalent, and computed inflow at various time intervals.\n\nThe POR data for JST Dam was obtained from the (internal facing) Savannah District Water Management [website](https://esasw6kbaaa0101.sas.ds.usace.army.mil/). The data was copy-pasted from the historic data query into Notepad++ and saved as .txt file. The data needs to be delimited prior to use in R.\n\n### Data Import, Formatting, and Export\n\nThe following code blocks were taken from *ThurmondPOR_Data.R*. The first block below also serves as a format for scripting best practices. Commenting code is invaluable (comments are the only way I can remember what I did in the script, months later).\n\nThis section contains a commented header (note: *\\# Header \\#####* will name a section of code). It is good practice to clear the environmental at the start of a script. The necessary libraries are also called at the top of a script. In order to load a library, it has to have been previously downloaded. *install.packages* has been included and commented out as an example. The line *theme_USACE.r* is ggplot theme created by Brian [Breaker](\\share.hec.usace.army.mil\\breaker\\rTraining).\n\n```{r Code Heading and Libraries}\n#| warning: false\n# Thurmond POR data ############################################################\n# Inputs are raw text files copied from WM Site\n# Dan McGraw\n# 6-March-2024\n################################################################################\n\n## Clear workspace\nrm(list = ls(all.names = TRUE))\n\n## Load Libraries\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nsource(\"E:/R/theme_USACE.r\")\n```\n\nThe next step is to read the delimited test files containing the POR data. These are all saved in the same folder which is *pordir*. Note that the data should be in .csv or .txt files. R can read excel files, but it isn't as simple as read.csv/read.table\n\n```{r import data}\n# Read files ###################################################################\n# Locate directory/folder with POR data\npordir <- \"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/\"\n\n# Create file path of the POR txt files\nthur_elev_file <- paste0(pordir,\"ThurmondDL_3-6-2024_ELEV.txt\")\nthur_flows_file <- paste0(pordir,\"ThurmondDL_3-6-2024_NetInflow_Discharge.txt\")\nthur_stor_file <- paste0(pordir,\"ThurmondDL_3-6-2024_STOR.txt\")\nthur_gen_file <- paste0(pordir,\"ThurmondDL_3-6-2024_GEN.txt\")\n\n# Read text files\nthur_elev <- read.table(thur_elev_file)\nthur_flows <- read.table(thur_flows_file)\nthur_stor <- read.table(thur_stor_file)\nthur_gen <- read.table(thur_gen_file)\n\n```\n\nHeadings were not read in with the data. These were added after import based on existing user knowledge of the fields. This code block is using *dplyr* from the tidyverse package. Tidyverse works very well with the pipe operator (%\\>%). Think of this as:\n\n*new data name* \\<- *old data name* \\>input\\> *function*\n\n```{r rename cols}\nthur_elev <- thur_elev %>% rename(Date = V1,DOW = V2, Elev_29 = V3) \nthur_flows <- thur_flows %>% rename(Date = V1,DOW = V2, NetInflow = V3, Discharge = V4) \nthur_stor <- thur_stor %>% rename(Date = V1,DOW = V2, Storage = V3) \nthur_gen <- thur_gen %>% rename(Date = V1,DOW = V2, Gen = V3)\n```\n\nAn example of the *thur_flows* dataset is below. Note the data was downloaded in descending date, this will be important later.\n\n```{r head}\nhead(thur_flows)\n```\n\nThe day of the week was not significant for this analysis. It was dropped from the dataset.\n\n```{r}\nthur_elev <- thur_elev %>% select (-c(DOW))\nthur_flows <- thur_flows %>% select (-c(DOW))\nthur_stor <- thur_stor %>% select (-c(DOW))\n```\n\nThe date field will be very important for the analysis. R has read the date field as a character data-type. This means there is no numerical basis to the values (the dates are just words). The dates need to be converted a date data-type for this analysis. For example, once the dates are formatted correctly, they can be arranged by increasing date. This is done using the *as.Date* function. Errors can arise when the dates are not formatted uniformly. The function allows several \"try\" inputs for cases like that. Another breakdown of the pipe operator is shown above the code block:\n\n*thur_flows_updated* = *thur_flows_existing* + a new DATE column (*mutate*), called *DT*, that will go before *NetInflow*\n\n```{r dates}\nthur_flows <- thur_flows %>% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = NetInflow )\nthur_elev <- thur_elev %>% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Elev_29)\nthur_stor <- thur_stor %>% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Storage)\n```\n\nSee the difference:\n\n```{r test}\nclass(thur_flows$Date)\nclass(thur_flows$DT)\n```\n\nNow the dates can be arranged by increasing date.\n\n```{r dates incr}\nthur_elev <- thur_elev %>% arrange(DT)\nthur_flows <- thur_flows %>% arrange(DT)\nthur_stor <- thur_stor %>% arrange(DT)\n```\n\nAn advantage to using R instead of excel is the ability to do column-wide functions without click & drag. The elevation data needs to be in NAVD88. The conversion from NGVD29 to NAVD88 is: $$ NAVD88 = NGVD29 - 0.7$$\n\n```{r elev88}\n# Create a new dataframe (df) for NAVD88 elevation\nthur_elev88 <- thur_elev %>% mutate(Elev_88 = Elev_29 - 0.7)\n\n# Drop the ngvd29 column from the navd88 df to avoid confusion\nthur_elev88 <- thur_elev88 %>% select(-c(Elev_29))\n```\n\nAt this point of the analysis the data was exported as csv files. These csv files could serve as a starting point for analysis in different scripts, without having to clean up the data as much (date will always need to be formatted *as.Date*). Note that an R & DSS package exists: <https://github.com/eheisman/dssrip>. The *write.csv* function has been commented out so the code will not export any files during this walkthrough.\n\n```{r export}\nthur_elev_outfile <- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV.csv\")\nthur_elev88_outfile <- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV_88.csv\")\nthur_flows_outfile <- paste0(pordir,\"csv/Thurmond_3-6-2024_NetInflow_Discharge.csv\")\nthur_stor_outfile <- paste0(pordir,\"csv/Thurmond_3-6-2024_STOR.csv\")\n\n#write.csv(thur_elev,thur_elev_outfile)\n#write.csv(thur_elev88,thur_elev88_outfile)\n#write.csv(thur_flows,thur_flows_outfile)\n#write.csv(thur_stor,thur_stor_outfile)\n```\n\n### Historic Inflows & Pool Elevations\n\nAn important first step of data analysis is understanding what the data looks like. R was used to quickly provide historic pool levels, large inflows, and additional time series context.\n\n```{r}\n#| warning: false\n# What years are contained within the POR\nthryears <- unique(year(thur_flows$DT))\n\n# What does the POR data look like\nggplot(data = thur_flows,aes(x=DT,y=Discharge)) + geom_point()+theme_USACE()\n```\n\nHow many inflows above 80,000 cfs (arbitrary) have there been?\n\n```{r}\n#| warning: false\nthur_flows %>% filter(NetInflow > 80000)%>%\n  ggplot(aes(x=DT,y=NetInflow))+geom_point() + theme_USACE()\n```\n\nThe top 10 largest pool elevations and inflows can be obtained as follows:\n\n```{r}\n# MAX Pool Elevations (top 10)\nthur_elev %>% top_n(10,Elev_29)%>% arrange(desc(Elev_29))\n```\n\n```{r}\n# MAX Pool Elevations (top 10)\nthur_flows %>% top_n(10,NetInflow) %>% arrange(desc(NetInflow))\n```\n\n### Creating Annual Maxima Data\n\nHydrologic analysis uses annual maxima series (AMS) based on water years (starting on October 1). Using the correctly formatted date column, an AMS can be created for inflow and reservoir pool elevation/stage. Note that the reservoir inflow is regulated. The first step is determining the water year of each data point.\n\n```{r water year}\n# Create separate columns of Year, Month, Day as numbers\nthur_flows <- thur_flows %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\n\n# ifelse is a binary yes/no operator. If the month greater than 10, WY = Yr+1\nthur_flows <- thur_flows %>% mutate(WaterYear = ifelse(Mon >= 10, Yr + 1, Yr))\n\nthur_elev88 <- thur_elev88 %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\nthur_elev88 <- thur_elev88 %>% mutate(WaterYear = ifelse(Mon >= 10, Yr + 1, Yr))\n\n```\n\nNow that the data can be grouped by water year, the annual max of each water year can be obtained\n\n```{r ams}\nannual_max_flows <- thur_flows %>%\n  group_by(WaterYear) %>%\n  summarize(MaxInflow = max(NetInflow),\n            Date = DT[which.max(NetInflow)],\n            Month = month(DT[which.max(NetInflow)]),\n            Day = day(DT[which.max(NetInflow)]))\n\nannual_max_stage88 <- thur_elev88 %>%\n  group_by(WaterYear) %>%\n  summarize(Max_Elev = max(Elev_88),\n            Date = DT[which.max(Elev_88)],\n            Month = month(DT[which.max(Elev_88)]),\n            Day = day(DT[which.max(Elev_88)]))\n\n# export if necessary\n# write.csv(annual_max_stage88,paste0(pordir,\"Stage_AMS.csv\"))\n# write.csv(annual_max_flows,paste0(pordir,\"Reg_Flow_AMS.csv\"))\n```\n\n### Critical Inflow Duration Analysis\n\nThis section of code aimed to determine the distribution of inflow durations at JST Dam. The procedure used was from from RMC-TR-2018-03 - \"Hydrologic Hazard Methodology for Semi-Quantitative Risk Assessments\". The TR defines *critical inflow duration* as the inflow duration that results in the highest water surface elevations for the reservoir of interest.\n\nAccording to RMC-TR-2018-03, three to 5 (3-5) historical peak reservoir events should be identified. The inflow, discharge, and stage for these events should be used to determine the critical inflow duration. The TR states \"select events that are consistent with the types of events likely to be the driver of extreme peak stages\".\n\nDue to the highly regulated nature of JST dam, the driver of extreme stage could vary from shorter, localized inflows to the dam OR longer, basin-wide inflows to the dam. In order to reach an understanding of basin operations as well as a good starting point for the critical inflow duration, the following analyses were done:\n\n#### Analyze the reservoir elevation rate of change\n\n1.  Determine the daily rate of change of reservoir elevation (ft/day)\n2.  Identify the top 150 daily rates (increasing)\n\n```{r wse rate}\n# Create and Set WSE Rate column to 0\nthur_elev88$WSERate <- NA\nthur_elev88$WSERate[1]<-0\n\n# Get rate of changer per day\nfor (i in 2:length(thur_elev88$Elev_88)){\n  day2 = thur_elev88$Elev_88[i]\n  day1 = thur_elev88$Elev_88[i-1]\n  stagedelta = day2 - day1\n  thur_elev88$WSERate[i] = stagedelta\n}\n\n# Select the top 150 rates\nFastRise <- thur_elev88 %>%\n  arrange(desc(WSERate)) %>% slice_max(WSERate,n=150)\n\n# Plot the rates against their corresponding reservoir elevations\nggplot(data=FastRise,aes(x=WSERate,y=Elev_88))+geom_point()+theme_USACE()+\n  labs(x = \"Max Daily Rate of Change (ft/day)\", y = \"Peak Reservoir Elev (NAVD88)\")\n\n```\n\nThis is helpful, but it can only tell so much since the inflow data is daily.\n\n#### Determine the inflow duration corresponding to the reservoir pool AMS\n\n1.  Loop through all high pool events and obtain corresponding inflow and discharge hydrographs from 7 days before and after\n2.  Find days during event where inflow \\> discharge\n3.  Save the hydrograph image and table\n4.  Create distribution of inflow durations\n\nThis code will save a csv and png of the inflow event\n\n```{r crit inflow}\n# NOTE annual_max_stage88$Date is a formatted date column\nams_dates <- annual_max_stage88$Date\n# ams_dates <- annual_max_flows$Date\n\neventDTs <- ams_dates\n\n# Grab flow hydrographs 14 days before and after max pool\neventDTs_start <- eventDTs - 7\neventDTs_end <- eventDTs + 7\n\nthur_rep_hydrographs <- list()\nthur_flood_durations <- list()\nflow.colors <- c(\"Inflow\" = \"#333BFF\", \"Outflow\" = \"orangered2\")\n\nfor (i in 1:length(eventDTs)){\n  hist_pool_date <- eventDTs[i]\n  \n  dt1 <- which(thur_flows$DT == eventDTs_start[i])\n  dt2 <- which(thur_flows$DT == eventDTs_end[i])\n  \n  if (eventDTs_start[i] < thur_flows$DT[1]){\n    dt1 <- 1\n  }\n  \n  hydrograph <- thur_flows[dt1:dt2,]\n  hydrograph$Event <- eventDTs[i]\n  thur_rep_hydrographs[[i]] <- hydrograph\n  \n  plot_date <- as.character(format(hist_pool_date, \"%d-%b-%Y\"))\n  flood_dur_plot<-ggplot(data = hydrograph)+\n    geom_line(aes(x=DT,y=NetInflow,color=\"Inflow\"))+geom_point(aes(x=DT,y=NetInflow,color=\"Inflow\"))+\n    geom_line(aes(x=DT,y=Discharge,color=\"Outflow\"))+geom_point(aes(x=DT,y=Discharge,color=\"Outflow\"))+\n    theme_USACE()+\n    ggtitle(paste0(\"Record Pool Event on \",plot_date))+\n    scale_x_date(date_breaks = \"1 day\")+labs(x=\"Date\",y=\"Discharge(cfs)\",color = \"Legend\")+\n    scale_color_manual(values = flow.colors)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n    theme(plot.title = element_text(hjust = 0.5))+\n    theme(legend.position = c(0.8,0.8))\n  \n  # Find where Inflow > outflow for flood duration\n  flood_dates <- hydrograph$DT[hydrograph$NetInflow >= hydrograph$Discharge]\n  peakflowdate <- hydrograph$DT[max(hydrograph$NetInflow) == hydrograph$NetInflow]\n  event_index <- which(flood_dates == peakflowdate)\n  \n  if (length(event_index) == 0){\n    event_index <- length(flood_dates)+3\n  }\n  \n  # Find consecutive dates around eventDT\n  start_index <- event_index\n  end_index <- event_index\n  \n  # Expand the start index to include previous consecutive dates\n  while (start_index > 1 && flood_dates[start_index] - flood_dates[start_index - 1] == 1) {\n    start_index <- start_index - 1\n  }\n  \n  # Expand the end index to include following consecutive dates\n  while (end_index < length(flood_dates) && flood_dates[end_index + 1] - flood_dates[end_index] == 1) {\n    end_index <- end_index + 1\n  }\n  \n  # Extract consecutive dates\n  consecutive_dates <- flood_dates[start_index:end_index]\n  \n  # Flood Duration in Days\n  flood_dur <- length(consecutive_dates)\n  \n  # march 21 2006 is tough, duration likely = 4-5 but outflow was always larger than inflow\n  # peak outflow matched peak inflow\n  \n  thur_flood_durations[[i]] <- data.frame(Event = hist_pool_date,Dur_days = flood_dur)\n\n  # Write CSV - uncomment to save\n  #write.csv(hydrograph,paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Flood_\",hist_pool_date,\".csv\"),row.names = F)\n  \n  # Save plot of flood event - uncomment to save \n  #ggsave(paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Plots/\",hist_pool_date,\".png\"),\n         #flood_dur_plot,width = 8, height = 6,dpi = 300)\n}\n\n# Convert the results to a usable df\nthur_dur <- do.call(rbind.data.frame,thur_flood_durations)\n```\n\n*thur_dur* contains the inflow duration of each high pool event. This was plotted as a histogram with a binwidth of 1-day to determine an initial estimate for the critical inflow duration.\n\n```{r inflow stats}\nmdur <- mean(thur_dur$Dur_days)\nsddur <- sd(thur_dur$Dur_days)\nmeddur <- median(thur_dur$Dur_days)\n\n# Base R does not have a Mode function.\nMode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Compute the mode\nprint(paste0(\"The mode is: \",Mode(thur_dur$Dur_days),\" days\"))\n```\n\n```{r plot colors}\n#| echo: false\ncolorpal <- brewer.pal(9,\"Set1\")\nhistcolor <- colorpal[2]\nlinecolor<- \"white\"#colorpal[9]\nmeancolor <- colorpal[1]\n#medcolor <- colorpal[3]\nsdcolor <- colorpal[3]\n```\n\nThe resulting hyetograph is shown below.\n\n```{r duration hist}\n#| warning: false\n\n# setting up a ggplot looks worse than it is\nggplot(data = thur_dur,aes(x=Dur_days))+\n  geom_histogram(binwidth=1,fill=histcolor,color=linecolor,alpha=0.9)+\n  scale_x_continuous(breaks = seq(1,14,1))+\n  theme_USACE()+\n  labs(x = \"Flood Duration (Days)\",y=\"Count\",\n       subtitle = \"Based on AMS Flood Events (1962 - 2024)\")+\n  ggtitle(\"Distribution of Flood Durations at Thurmond Dam\")+\n  geom_vline(xintercept = mdur, color=meancolor)+\n  geom_vline(xintercept = (mdur+sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_vline(xintercept = (mdur-sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_text(aes(x=mdur, label=paste(\"\\nMean = \",round(mdur,2),sep=\"\"),\n                y=7.5), colour=meancolor, angle=90)+\n  geom_text(aes(x=(mdur+sddur), label=paste(\"\\n+1SD = \",round(mdur+sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  geom_text(aes(x=(mdur-sddur), label=paste(\"\\n-1SD = \",round(mdur-sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n```\n\nBased on the results of the AMS duration distribution, the critical inflow duration is likely 3-, 4-, or 5-days. With this in mind, five historical peak reservoir events were identified to estimate a final critical duration. The loop used to create the inflow duration distribution contained a code to export a csv and png of each AMS event. These were used to estimate a final critical inflow duration. This has been replicated below.\n\n```{r final crit duration}\nevent <- c(\"11-Apr-1964\",\"30-Mar-1964\",\"3-Mar-1971\",\"17-Mar-1975\",\"7-Feb-1998\",\"1-Jan-2016\")\nduration <- c(5,4,4,4.5,4.5,4)\n\nmean_inflow_dur <- round(mean(duration),2)\ngeomean_inflow_dur <- round(exp(mean(log(duration))),2)\n\ncat(\"The mean inflow duration is \",mean_inflow_dur,\"\\nThe geometric mean inflow duration is \", geomean_inflow_dur)\n\n```\n\nBoth 4-day and 5-day could be considered for the critical inflow duration. During the project, the 4-day inflow was selected as the critical duration.\n\nAs a preemptive analysis of seasonallity, the stage seasonallity was analyzed.\n\n```{r stage season}\n#| warning: false\nggplot(data = annual_max_stage88,aes(x = Month))+\n  geom_histogram(binwidth=1,fill=\"lightblue\",color=\"black\",alpha=0.9)+\n  stat_density(aes(y = ..count..), geom = \"line\", color = \"green3\", size = 1) +\n  scale_x_continuous(breaks = seq(1,12,1),\n                     labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"June\",\"July\",\"Aug\",\"Sept\",\"Oct\",\"Nov\",\"Dec\"))+\n  scale_y_continuous(breaks = seq(0,20,2),minor_breaks = seq(0,20,1))+\n  theme_USACE()+\n  labs(x = \"Month\",y=\"Count\",subtitle = \"Based on AMS of POR Stage Elev.(1962 - 2024)\")+\n  ggtitle(\"Flood Seasonality at Thurmond Dam\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))\n```\n\n## Unregulated 4-day Inflow AMS\n\nThe next steps were to create a timeseries of unregulated inflow data that can be used to create an AMS of 4-day inflow volumes. This section incorporates code used in *CreatingUnregulatedData_Augusta.R* and *Crtical Volume Duration.R*.\n\nThe Augusta USGS Streamgage was the subject of a 1990 USACE & USGS report that analyzed the relationship between regulated and unregulated flow-frequency estimates. Due to the existence of this study and the longer gage record, the Augusta gage was used as the primary data source for inflow data. The overlapping record of the Augusta gage with the former USGS streamgage at JST Dam was used to verify the drainage area ratio from the design memo (0.93).\n\n$$ Q_T = 0.93 * Q_A$$\n\nIn order to create a 4-day inflow volume annual maximum series (AMS), the daily streamflow data at the Augusta gage was processed using the following steps:\n\n1.  The effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\n\n2.  Daily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in Figure 14 (PA Report - Chapter 4). This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n\n3.  4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n\n4.  4-day unregulated volume AMS was obtained for each water year\n\n5.  The peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\n\n6.  The peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\n\n7.  The drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\nThese steps & figures will be replicated below.\n\n### Unregulated Inflow Data\n\nBefore any of the steps above, the data should be imported and formatted.\n\n```{r}\n#| warning: false\n# Import Augusta Daily & Format Dates\nlibrary(\"dataRetrieval\")\n\n# Import using Augusta Site Number\nsiteNo <- \"02197000\"\npCode <- \"00060\" #00061 - # Check codes from USGS\nstart.date <- \"1700-01-01\" # arbitraty to get all info\nend.date <- \"2024-12-31\"\n\naugusta_daily <- readNWISdv(siteNumbers = siteNo,\n                            parameterCd = pCode,\n                            startDate = start.date,\n                            endDate = end.date)\n\naugusta_daily <- renameNWISColumns(augusta_daily)\n\naugusta_daily <- augusta_daily %>% mutate(DT = as.Date(Date),.before = Flow)\n\n# Add water year column\naugusta_daily <- augusta_daily %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_daily <- augusta_daily %>% mutate(WaterYear = ifelse(Mon > 9, Yr + 1, Yr))\n\n# Make sure the data is there\nggplot(data = augusta_daily) + geom_line(aes(x=DT,y=Flow)) + theme_bw()\n\n```\n\nThe regulated-unregulated relationship was digitized from the 1990 report and imported into R.\n\n```{r unreg-reg}\n# Import Digitized Data for Figure 35 (1990) - Exceedance Prob for Unregulated and Regulated\nfig35 <- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Figure35data.csv\")\nfig35 <- fig35 %>% mutate(NEP = 100-ExceedanceProb)\n\n# Convert Reg and Unreg at Augusta\naugusta_conversion <- fig35 %>% mutate(Aug_RegQ = PeakQRegulated,Aug_UnregQ = PeakQUnregulated)\naugusta_conversion <- augusta_conversion %>% select(c(Aug_RegQ,Aug_UnregQ))\n\n# Recreate Figure 35\nlogscalebreaks = c(1000,5000,10000,20000,50000,100000,200000,500000,1000000)\nlogscaleminor_breaks = c(seq(2000, 9000, by = 1000),seq(20000, 90000, by = 10000),\n                         seq(200000, 900000, by = 100000))\n\nggplot(data=fig35,aes(x=PeakQUnregulated,y=PeakQRegulated))+\n  geom_point()+\n  geom_line()+\n  theme_USACE()+\n  ggtitle(\"Savannah River at Augusta, GA (02197000)\")+\n  labs(x=\"Unregulated Discharge (cfs)\",y=\"Regulated Discharge (cfs)\",subtitle = \"Relation between regulated peak discharges and unregulated peak discharges\",\n       color = \"Legend\")+\n  theme(plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5),legend.position = \"bottom\")+\n  geom_abline(intercept = 0, slope = 1, color = \"blue1\", linetype = \"dashed\") +\n  scale_y_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  scale_x_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  coord_cartesian(xlim = c(5000,1000000),ylim = c(5000,1000000))\n```\n\nThe Peak Flow dataset at Augusta will also be used. This data was obtained from the USGS website. Additional post processing was done to make this easier (i.e. some manual entry to Flow Notes)\n\n```{r peak flow}\n# Import and Format\naugusta_peak_ams <- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Augusta Gage/csv/Augusta_Peak_AMS.csv\",header = T)\naugusta_peak_ams <- augusta_peak_ams %>% mutate(DT = as.Date(Date),.before = Flow)\naugusta_peak_ams <- augusta_peak_ams %>% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_peak_ams <- augusta_peak_ams %>% mutate(WaterYear = ifelse(Mon > 9, Yr + 1, Yr))\naugusta_peak_ams <- augusta_peak_ams %>% rename(Peak_Flow = Flow)\n```\n\n1.  The effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\n\n```{r start of reg}\n# Regulation Assumed to start in 1951\nregulation_wy <- 1951\npre_reg <- augusta_daily %>% filter(WaterYear < regulation_wy)\naug_reg <- augusta_daily %>% filter(WaterYear >= regulation_wy)\n\npre_reg_peak <- augusta_peak_ams %>% filter(WaterYear < regulation_wy)\naug_reg_peak <- augusta_peak_ams %>% filter(WaterYear >= regulation_wy)\n```\n\n2.  Daily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in the relationship above. This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n\n```{r}\n# Estimate Unregulated from approx function\nAugusta_Unreg <- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ, xout = aug_reg$Flow)[2]\nAugusta_Unreg_peak <- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ,\n                             xout=aug_reg_peak$Peak_Flow)[2]\n\n# add to regulated data\naug_reg <- aug_reg %>% mutate(Unregulated_Flow = round(Augusta_Unreg$y,0))\naug_reg_peak <- aug_reg_peak %>% mutate(Unregulated_PeakFlow = round(Augusta_Unreg_peak$y,0))\n\n# create a 1:1 column of unregulated flow to make bind_rows easier\npre_reg <- pre_reg %>% mutate(Unregulated_Flow = round(Flow,0))\npre_reg_peak <- pre_reg_peak %>% mutate(Unregulated_PeakFlow = round(Peak_Flow,0))\n\n# Combine datasets\naugusta_daily_unreg <- bind_rows(pre_reg,aug_reg)\nAMS_Peak <- bind_rows(pre_reg_peak,aug_reg_peak)\n\n# remove any confusion\nrm(Augusta_Unreg)\nrm(Augusta_Unreg_peak)\n\n# Check results for sanity \nggplot(data = augusta_daily_unreg) +\n  geom_line(aes(x=DT,y=Unregulated_Flow,color = \"Unregulated\"))+\n  geom_line(aes(x=DT,y=Flow,color=\"Existing\"))+\n  theme_USACE()+\n  labs(x = \"Date\",\n       y = \"Daily Flow (cfs)\",\n       title = \"Daily Flow Values Augusta (02197000)\",\n       subtitle = \"Post-WY 1951 Converted to Unregulated\")+\n  scale_y_continuous(breaks = seq(0,250000,50000),minor_breaks = seq(0,250000,10000))+\n  scale_x_date(breaks = \"10 year\",minor_breaks = \"1 year\")+\n  scale_color_manual(values = c(\"#377EB8\", \"red2\"), \n                     name = \"Legend\", \n                     labels = c(\"Existing\",\"Unregulated\"))+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),legend.position = c(.85,.8))\n```\n\n3.  4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n\n```{r 4 day vols}\n# Create Data with gaps to avoid rolling mean over the gaps (i.e. 1891-12-31 & 1896-01-01)\ncomplete_dates <- seq(min(augusta_daily_unreg$DT), max(augusta_daily_unreg$Date), by = \"day\")\naug_unreg_complete <- merge(augusta_daily_unreg, data.frame(Date = complete_dates), by = \"Date\", all = TRUE)\n# aug_unreg_complete[3010:3020,]  # check\n#naniar::vis_miss(aug_unreg_complete) # see gaps in data, uncomment to use\n\ncritdur <- 4\n# rollmean is a 4 day rolling average. rollmeanr indicates that the window is aligned to the \"right\"\n# This will compute a time series of 4-day inflow volumes\naug_unreg_complete <- aug_unreg_complete %>% mutate(VolDur_4day = round(rollmeanr(Unregulated_Flow,k = critdur, fill=NA),0))\n\n```\n\n4.  4-day unregulated volume AMS was obtained for each water year\n\n```{r 4 day ams}\nAMS_4day <- aug_unreg_complete %>%\n  # Group by WY\n  group_by(WaterYear) %>%\n  # Take the largest value from each WY\n  top_n(1, VolDur_4day) %>%\n  # Ensures no duplicate years\n  distinct(WaterYear, .keep_all = TRUE) %>%\n  ungroup()\n\n# Combine Peak and 4-day AMS to help stay organized\nall_AMS <- left_join(AMS_Peak,AMS_4day,by = \"WaterYear\")\ncolnames(all_AMS)\nall_AMS <- all_AMS %>% select(c(DT.x,Peak_Flow,Flow_notes,WaterYear,Unregulated_PeakFlow,DT.y,Unregulated_Flow,VolDur_4day))\nall_AMS <- all_AMS %>% rename(DT_Peak = DT.x, DT_Daily = DT.y)\n```\n\n5.  The peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\n\n```{r peak to vol}\n# Find the peaks and 4-day vols that are from the same event. 4 is the search threshold then\nmatchingAMS <- all_AMS %>% filter(DT_Peak >= DT_Daily - 4 | DT_Peak <= DT_Daily + 4)\nmatchingAMS <- matchingAMS %>% mutate(PeaktoVol = Unregulated_PeakFlow/VolDur_4day)\n\n# Now Add this peak to vol data back to the AMS dataset\nall_AMS <- all_AMS %>% mutate(Ratio_Peak_4d = ifelse(DT_Peak >= DT_Daily - 4 | DT_Peak <= DT_Daily + 4,Unregulated_PeakFlow/VolDur_4day,NA))\n\ncat(\"The mean peak-to-volume ratio is \",round(mean(matchingAMS$PeaktoVol),2),\"\\nThe geometric mean peak-to-volume ratio is \", round(exp(mean(log(matchingAMS$PeaktoVol))),2),\"\\nThe median peak-to-volume ratio is \",round(median(matchingAMS$PeaktoVol),2))\n```\n\n```{r peak to vol plot}\nggplot(data = matchingAMS)+\n  geom_histogram(aes(x =PeaktoVol),fill=\"lightblue\",color=\"black\",alpha=0.9,\n                 breaks = seq(0.9,3.0,0.1))+\n  theme_USACE()+\n  ggtitle(\"Ratio of Peak Flow to 4-Day Volume at Augusta (02197000)\")+\n  labs(x=\"Peak Flow/4-Day Vol\", y=\"Count\")+\n  scale_x_continuous(breaks = seq(0.9,3.0,0.1),minor_breaks = seq(0,3.0,0.05))+\n  scale_y_continuous(breaks = seq(0,25,1))+\n  geom_vline(xintercept = mean(matchingAMS$PeaktoVol), color=\"orange2\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+\n  theme(axis.text.x = element_text(size=6.5))\n```\n\n6.  The peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\n\n```{r pvt missing vals}\n# Peak to volume ratio\nPtVratio_4day <- mean(matchingAMS$PeaktoVol)\n\n# Calculate missing 4 day volumes\nall_AMS <- all_AMS %>% mutate(Peak_to_4day = round(Unregulated_PeakFlow/PtVratio_4day,0))\n\nall_AMS <- all_AMS %>% mutate(Complete_4Day_AMS = ifelse(is.na(all_AMS$VolDur_4day),Peak_to_4day,VolDur_4day))\n```\n\n7.  The drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\n```{r Thurmond DA ratio}\n# Previously calculated drainage area ratio\nDAratio <- 0.93\n\n# Multiply the Augusta AMS values by the DA ratio\nall_AMS <- all_AMS %>% mutate(Thurm_AMS_4day = round(DAratio*Complete_4Day_AMS,0))\n```\n\nBONUS STEP: Prepare the data to be used in BestFit. This will use the flow notes to set perception threshold values & interval values (assumed to be +/- 20%). The interval calculation can be tough to decipher via code. It has been written out as an equation below.\n\n$$ Interval_L = 0.80 * 0.93 * PeakQ/PeakToVol$$ $$ Interval_U = 1.20 * 0.93 * PeakQ/PeakToVol$$\n\n```{r Best Fit Prep}\n# Rename some columns to make it easier - this is because my variable naming was not as intuitive as it should have been\nall_AMS <- all_AMS %>% rename(Aug_AMS_1day = Unregulated_Flow, \n                              Aug_AMS_4day = VolDur_4day,\n                              Aug_Complete_4Day_AMS = Complete_4Day_AMS)\n\n# If there is no peak to 4day ratio, this was historic peak flow information\nall_AMS <- all_AMS %>% mutate(BestFitNotes = ifelse(is.na(Ratio_Peak_4d),\"Estimated from Peak AMS\",NA))\n\n# Computing intervals. ifelse ensures it only computs missing ones\nall_AMS <- all_AMS %>% mutate(Interval_Lower =\n                                ifelse(WaterYear<1876,round((DAratio*0.8*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_MostLikely =                                 \n                                ifelse(WaterYear<1876,round((DAratio*1.0*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_Upper =\n                                ifelse(WaterYear<1876,round((DAratio*1.2*Unregulated_PeakFlow)/PtVratio_4day,0),NA))\n```\n\nThe *allAMS* dataframe can be exported to .csv or it can be further filtered to only include fields required for BestFit.\n\n```{r export bestfit}\n# Exporting allAMS - commented out\n# write.csv(all_AMS,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_AMS_Peak_1day_4day.csv\",row.names = F)\n\n# Selecting fields from allAMS that are required for BestFit\nbestfit_input <- all_AMS %>% select(c(WaterYear,Peak_Flow,Flow_notes,Unregulated_PeakFlow,\n                                      Thurm_AMS_4day,BestFitNotes,\n                                      Interval_Lower,Interval_MostLikely,Interval_Upper))\n\n# Export BestFit to csv - commented out\n#write.csv(bestfit_input,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_BestFitINPUT.csv\",row.names = F)\n\n```\n\n## Theme USACE\n\nMove this before the other code blocks for ggplot themes. If issues continue, remove *+theme_USACE()* from the ggplot chunks\n\n```{r theme USACE}\ntheme_USACE <-  function(base_size = 8){theme(\n  text = element_text(family = 'serif', color = 'black'),\n  line = element_line(colour = 'black', linewidth = 0.2), \n  rect = element_rect(colour = 'black', linewidth = 0.2),\n  plot.title = element_text(vjust = 3, size = 9),\n  plot.margin = unit(c(1,1,1,1), 'lines'),\n  panel.border = element_rect(fill = F),\n  panel.grid.major = element_line(colour = 'grey50', linewidth = 0.2),\n  panel.grid.minor = element_line(colour = 'grey75', linewidth = 0.1),\n  panel.background = element_rect(fill = 'white'),\n  #defaults legend to upper left, can/should be overridden based on graph\n  #legend.background = element_blank(),\n  legend.background = element_rect(fill = \"lightgrey\", colour = \"black\"),\n  legend.justification = c(\"left\", \"top\"),\n  legend.position = c(0.8, 0.5),\n  # this value should be adjusted dependent on \n  # graph with the addition of another \n  # theme(legend.position = c(X, Y)) argument after theme_USACE()\n  # or... for no legend \n  # legend.position = element_blank(),\n  legend.key = element_blank(),\n  legend.title = element_text(size = 9),\n  #legend.title = element_blank(), \n  axis.title.x = element_text(size = 9),\n  axis.title.y = element_text(angle = 90, size = 9),\n  axis.text.x = element_text(margin = margin(8, 0, 0, 0)),\n  axis.text.y = element_text(margin = margin(0, 8, 0, 0)),\n  axis.ticks.length = unit(0.25 , 'cm')\n)}\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"JST_inR.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.9","title":"Thurmond PA - R Scripting","author":[{"name":"Dan McGraw","affiliation":"USACE SAS EN-H","email":"daniel.e.mcgraw@usace.army.mil"}],"date":"2024-08-08","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}