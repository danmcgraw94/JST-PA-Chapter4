[
  {
    "objectID": "JST_inR.html",
    "href": "JST_inR.html",
    "title": "Thurmond PA - R Scripting",
    "section": "",
    "text": "This walkthrough is to provide a description of R scripts/files used in the development of the J. Strom Thurmond Dam Hydrologic Hazard Curve (HHC). The readme files contained in the project folder provide a summary of various scripts. Algorithim development is an iterative process, so there was not a singular centralized script. The document will incorporate the contents of several scripts developed for the HHC. R is an open source, free software. All the libraries/functions used have extensive information online on their use and trouble shooting.\n\n\nData requests for existing period of record (POR) data should be completed through the home district’s water management section. Typical data stored for projects throughout the USACE portfolio range from observed lake elevation, stage, flow, precipitation, temperature, snow depth, snow water equivalent, and computed inflow at various time intervals.\nThe POR data for JST Dam was obtained from the (internal facing) Savannah District Water Management website. The data was copy-pasted from the historic data query into Notepad++ and saved as .txt file. The data needs to be delimited prior to use in R.\n\n\nThe following code blocks were taken from ThurmondPOR_Data.R. The first block below also serves as a format for scripting best practices. Commenting code is invaluable (comments are the only way I can remember what I did in the script, months later).\nThis section contains a commented header (note: # Header ##### will name a section of code). It is good practice to clear the environmental at the start of a script. The necessary libraries are also called at the top of a script. In order to load a library, it has to have been previously downloaded. install.packages has been included and commented out as an example. The line theme_USACE.r is ggplot theme created by Brian Breaker.\n\n# Thurmond POR data ############################################################\n# Inputs are raw text files copied from WM Site\n# Dan McGraw\n# 6-March-2024\n################################################################################\n\n## Clear workspace\nrm(list = ls(all.names = TRUE))\n\n## Load Libraries\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nsource(\"E:/R/theme_USACE.r\")\n\nThe next step is to read the delimited test files containing the POR data. These are all saved in the same folder which is pordir. Note that the data should be in .csv or .txt files.\n\n# Read files ###################################################################\n# Locate directory/folder with POR data\npordir &lt;- \"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/\"\n\n# Create file path of the POR txt files\nthur_elev_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_ELEV.txt\")\nthur_flows_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_NetInflow_Discharge.txt\")\nthur_stor_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_STOR.txt\")\nthur_gen_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_GEN.txt\")\n\n# Read text files\nthur_elev &lt;- read.table(thur_elev_file)\nthur_flows &lt;- read.table(thur_flows_file)\nthur_stor &lt;- read.table(thur_stor_file)\nthur_gen &lt;- read.table(thur_gen_file)\n\nHeadings were not read in with the data. These were added after import based on existing user knowledge of the fields. This code block is using dplyr from the tidyverse package. Tidyverse works very well with the pipe operator (%&gt;%). Think of this as:\nnew data name &lt;- old data name &gt;input&gt; function\n\nthur_elev &lt;- thur_elev %&gt;% rename(Date = V1,DOW = V2, Elev_29 = V3) \nthur_flows &lt;- thur_flows %&gt;% rename(Date = V1,DOW = V2, NetInflow = V3, Discharge = V4) \nthur_stor &lt;- thur_stor %&gt;% rename(Date = V1,DOW = V2, Storage = V3) \nthur_gen &lt;- thur_gen %&gt;% rename(Date = V1,DOW = V2, Gen = V3)\n\nAn example of the thur_flows dataset is below. Note the data was downloaded in descending date, this will be important later.\n\nhead(thur_flows)\n\n        Date DOW NetInflow Discharge\n1 03/05/2024 Tue     10412      6177\n2 03/04/2024 Mon     15313      6490\n3 03/03/2024 Sun      6656      6656\n4 03/02/2024 Sat      6341      6694\n5 03/01/2024 Fri      7259      4436\n6 02/29/2024 Thu      2606      4371\n\n\nThe day of the week was not significant for this analysis. It was dropped from the dataset.\n\nthur_elev &lt;- thur_elev %&gt;% select (-c(DOW))\nthur_flows &lt;- thur_flows %&gt;% select (-c(DOW))\nthur_stor &lt;- thur_stor %&gt;% select (-c(DOW))\n\nThe date field will be very important for the analysis. Initially, it is common for R to read the date field as a character data-type. This means there is no numerical basis to the values (the dates are just words). The dates need to be converted a date data-type for this analysis. For example, once the dates are formatted correctly, they can be arranged by increasing date. This is done using the as.Date function. Errors can arise when the dates are not formatted uniformly. The function allows several “try” inputs for cases like that. Another breakdown of the pipe operator is shown above the code block:\nthur_flows_updated = thur_flows_existing + a new DATE column (mutate), called DT, that will go before NetInflow\n\nthur_flows &lt;- thur_flows %&gt;% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = NetInflow )\nthur_elev &lt;- thur_elev %&gt;% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Elev_29)\nthur_stor &lt;- thur_stor %&gt;% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Storage)\n\nSee the difference:\n\nclass(thur_flows$Date)\n\n[1] \"character\"\n\nclass(thur_flows$DT)\n\n[1] \"Date\"\n\n\nNow the dates can be arranged by increasing date.\n\nthur_elev &lt;- thur_elev %&gt;% arrange(DT)\nthur_flows &lt;- thur_flows %&gt;% arrange(DT)\nthur_stor &lt;- thur_stor %&gt;% arrange(DT)\n\nAn advantage of using R instead of excel is the ability to do column-wide functions without click & drag. The elevation data needs to be in NAVD88. The conversion from NGVD29 to NAVD88 is: \\[ NAVD88 = NGVD29 - 0.7\\]\n\n# Create a new dataframe (df) for NAVD88 elevation\nthur_elev88 &lt;- thur_elev %&gt;% mutate(Elev_88 = Elev_29 - 0.7)\n\n# Drop the ngvd29 column from the navd88 df to avoid confusion\nthur_elev88 &lt;- thur_elev88 %&gt;% select(-c(Elev_29))\n\nAt this point of the analysis the data was exported as csv files. These csv files could serve as a starting point for analysis in different scripts, without having to clean up the data as much (date will always need to be formatted as.Date). Note that an R & DSS package exists: https://github.com/eheisman/dssrip. The write.csv function has been commented out so the code will not export any files during this walkthrough.\n\nthur_elev_outfile &lt;- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV.csv\")\nthur_elev88_outfile &lt;- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV_88.csv\")\nthur_flows_outfile &lt;- paste0(pordir,\"csv/Thurmond_3-6-2024_NetInflow_Discharge.csv\")\nthur_stor_outfile &lt;- paste0(pordir,\"csv/Thurmond_3-6-2024_STOR.csv\")\n\n#write.csv(thur_elev,thur_elev_outfile)\n#write.csv(thur_elev88,thur_elev88_outfile)\n#write.csv(thur_flows,thur_flows_outfile)\n#write.csv(thur_stor,thur_stor_outfile)\n\n\n\n\nAn important first step of data analysis is understanding what the data looks like. R was used to quickly provide historic pool levels, large inflows, and additional time series context.\n\n# What years are contained within the POR\nthryears &lt;- unique(year(thur_flows$DT))\n\n# What does the POR data look like\nggplot(data = thur_flows,aes(x=DT,y=Discharge)) + geom_point()+theme_USACE()\n\n\n\n\n\n\n\n\nHow many inflows above 80,000 cfs (arbitrary) have there been?\n\nthur_flows %&gt;% filter(NetInflow &gt; 80000)%&gt;%\n  ggplot(aes(x=DT,y=NetInflow))+geom_point() + theme_USACE()\n\n\n\n\n\n\n\n\nThe top 10 largest pool elevations and inflows can be obtained as follows:\n\n# MAX Pool Elevations (top 10)\nthur_elev %&gt;% top_n(10,Elev_29)%&gt;% arrange(desc(Elev_29))\n\n         Date         DT Elev_29\n1  04/11/1964 1964-04-11  336.46\n2  04/10/1964 1964-04-10  336.31\n3  12/31/2015 2015-12-31  335.94\n4  01/01/2016 2016-01-01  335.94\n5  01/02/2016 2016-01-02  335.93\n6  04/09/1964 1964-04-09  335.73\n7  02/07/1998 1998-02-07  335.45\n8  05/06/1964 1964-05-06  335.40\n9  03/30/1964 1964-03-30  335.38\n10 02/06/1998 1998-02-06  335.37\n\n\n\n# MAX Pool Elevations (top 10)\nthur_flows %&gt;% top_n(10,NetInflow) %&gt;% arrange(desc(NetInflow))\n\n         Date         DT NetInflow Discharge\n1  03/03/1971 1971-03-03    116127      3634\n2  03/14/1975 1975-03-14    108026     14826\n3  04/08/1964 1964-04-08    104761     36554\n4  03/26/1964 1964-03-26    100266     14519\n5  02/04/1998 1998-02-04     99121     16127\n6  04/07/1964 1964-04-07     97413     12778\n7  03/27/1964 1964-03-27     92612     15964\n8  04/09/1964 1964-04-09     91136     70782\n9  01/26/1978 1978-01-26     90983     15167\n10 04/16/1969 1969-04-16     87295      6716\n\n\n\n\n\nHydrologic analysis uses annual maxima series (AMS) based on water years (starting on October 1). Using the correctly formatted date column, an AMS can be created for inflow and reservoir pool elevation/stage. Note that the reservoir inflow is regulated. The first step is determining the water year of each data point.\n\n# Create separate columns of Year, Month, Day as numbers\nthur_flows &lt;- thur_flows %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\n\n# ifelse is a binary yes/no operator. If the month greater than 10, WY = Yr+1\nthur_flows &lt;- thur_flows %&gt;% mutate(WaterYear = ifelse(Mon &gt;= 10, Yr + 1, Yr))\n\nthur_elev88 &lt;- thur_elev88 %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\nthur_elev88 &lt;- thur_elev88 %&gt;% mutate(WaterYear = ifelse(Mon &gt;= 10, Yr + 1, Yr))\n\nNow that the data can be grouped by water year, the annual max of each water year can be obtained\n\nannual_max_flows &lt;- thur_flows %&gt;%\n  group_by(WaterYear) %&gt;%\n  summarize(MaxInflow = max(NetInflow),\n            Date = DT[which.max(NetInflow)],\n            Month = month(DT[which.max(NetInflow)]),\n            Day = day(DT[which.max(NetInflow)]))\n\nannual_max_stage88 &lt;- thur_elev88 %&gt;%\n  group_by(WaterYear) %&gt;%\n  summarize(Max_Elev = max(Elev_88),\n            Date = DT[which.max(Elev_88)],\n            Month = month(DT[which.max(Elev_88)]),\n            Day = day(DT[which.max(Elev_88)]))\n\n# export if necessary\n# write.csv(annual_max_stage88,paste0(pordir,\"Stage_AMS.csv\"))\n# write.csv(annual_max_flows,paste0(pordir,\"Reg_Flow_AMS.csv\"))\n\n\n\n\nThis section of code aimed to determine the distribution of inflow durations at JST Dam. The procedure used was from from RMC-TR-2018-03 - “Hydrologic Hazard Methodology for Semi-Quantitative Risk Assessments”. The TR defines critical inflow duration as the inflow duration that results in the highest water surface elevations for the reservoir of interest.\nAccording to RMC-TR-2018-03, three to 5 (3-5) historical peak reservoir events should be identified. The inflow, discharge, and stage for these events should be used to determine the critical inflow duration. The TR states “select events that are consistent with the types of events likely to be the driver of extreme peak stages”.\nDue to the highly regulated nature of JST dam, the driver of extreme stage could vary from shorter, localized inflows to the dam OR longer, basin-wide inflows to the dam. In order to reach an understanding of basin operations as well as a good starting point for the critical inflow duration, the following analyses were done:\n\n\n\nDetermine the daily rate of change of reservoir elevation (ft/day)\nIdentify the top 150 daily rates (increasing)\n\n\n# Create and Set WSE Rate column to 0\nthur_elev88$WSERate &lt;- NA\nthur_elev88$WSERate[1]&lt;-0\n\n# Get rate of changer per day\nfor (i in 2:length(thur_elev88$Elev_88)){\n  day2 = thur_elev88$Elev_88[i]\n  day1 = thur_elev88$Elev_88[i-1]\n  stagedelta = day2 - day1\n  thur_elev88$WSERate[i] = stagedelta\n}\n\n# Select the top 150 rates\nFastRise &lt;- thur_elev88 %&gt;%\n  arrange(desc(WSERate)) %&gt;% slice_max(WSERate,n=150)\n\n# Plot the rates against their corresponding reservoir elevations\nggplot(data=FastRise,aes(x=WSERate,y=Elev_88))+geom_point()+theme_USACE()+\n  labs(x = \"Max Daily Rate of Change (ft/day)\", y = \"Peak Reservoir Elev (NAVD88)\")\n\n\n\n\n\n\n\n\nThis is helpful, but it can only tell so much since the inflow data is daily.\n\n\n\n\nLoop through all high pool events and obtain corresponding inflow and discharge hydrographs from 7 days before and after\nFind days during event where inflow &gt; discharge\nSave the hydrograph image and table\nCreate distribution of inflow durations\n\nThis code will save a csv and png of the inflow event\n\n# NOTE annual_max_stage88$Date is a formatted date column\nams_dates &lt;- annual_max_stage88$Date\n# ams_dates &lt;- annual_max_flows$Date\n\neventDTs &lt;- ams_dates\n\n# Grab flow hydrographs 14 days before and after max pool\neventDTs_start &lt;- eventDTs - 7\neventDTs_end &lt;- eventDTs + 7\n\nthur_rep_hydrographs &lt;- list()\nthur_flood_durations &lt;- list()\nflow.colors &lt;- c(\"Inflow\" = \"#333BFF\", \"Outflow\" = \"orangered2\")\n\nfor (i in 1:length(eventDTs)){\n  hist_pool_date &lt;- eventDTs[i]\n  \n  dt1 &lt;- which(thur_flows$DT == eventDTs_start[i])\n  dt2 &lt;- which(thur_flows$DT == eventDTs_end[i])\n  \n  if (eventDTs_start[i] &lt; thur_flows$DT[1]){\n    dt1 &lt;- 1\n  }\n  \n  hydrograph &lt;- thur_flows[dt1:dt2,]\n  hydrograph$Event &lt;- eventDTs[i]\n  thur_rep_hydrographs[[i]] &lt;- hydrograph\n  \n  plot_date &lt;- as.character(format(hist_pool_date, \"%d-%b-%Y\"))\n  flood_dur_plot&lt;-ggplot(data = hydrograph)+\n    geom_line(aes(x=DT,y=NetInflow,color=\"Inflow\"))+geom_point(aes(x=DT,y=NetInflow,color=\"Inflow\"))+\n    geom_line(aes(x=DT,y=Discharge,color=\"Outflow\"))+geom_point(aes(x=DT,y=Discharge,color=\"Outflow\"))+\n    theme_USACE()+\n    ggtitle(paste0(\"Record Pool Event on \",plot_date))+\n    scale_x_date(date_breaks = \"1 day\")+labs(x=\"Date\",y=\"Discharge(cfs)\",color = \"Legend\")+\n    scale_color_manual(values = flow.colors)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n    theme(plot.title = element_text(hjust = 0.5))+\n    theme(legend.position = c(0.8,0.8))\n  \n  # Find where Inflow &gt; outflow for flood duration\n  flood_dates &lt;- hydrograph$DT[hydrograph$NetInflow &gt;= hydrograph$Discharge]\n  peakflowdate &lt;- hydrograph$DT[max(hydrograph$NetInflow) == hydrograph$NetInflow]\n  event_index &lt;- which(flood_dates == peakflowdate)\n  \n  if (length(event_index) == 0){\n    event_index &lt;- length(flood_dates)+3\n  }\n  \n  # Find consecutive dates around eventDT\n  start_index &lt;- event_index\n  end_index &lt;- event_index\n  \n  # Expand the start index to include previous consecutive dates\n  while (start_index &gt; 1 && flood_dates[start_index] - flood_dates[start_index - 1] == 1) {\n    start_index &lt;- start_index - 1\n  }\n  \n  # Expand the end index to include following consecutive dates\n  while (end_index &lt; length(flood_dates) && flood_dates[end_index + 1] - flood_dates[end_index] == 1) {\n    end_index &lt;- end_index + 1\n  }\n  \n  # Extract consecutive dates\n  consecutive_dates &lt;- flood_dates[start_index:end_index]\n  \n  # Flood Duration in Days\n  flood_dur &lt;- length(consecutive_dates)\n  \n  # march 21 2006 is tough, duration likely = 4-5 but outflow was always larger than inflow\n  # peak outflow matched peak inflow\n  \n  thur_flood_durations[[i]] &lt;- data.frame(Event = hist_pool_date,Dur_days = flood_dur)\n\n  # Write CSV - uncomment to save\n  #write.csv(hydrograph,paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Flood_\",hist_pool_date,\".csv\"),row.names = F)\n  \n  # Save plot of flood event - uncomment to save \n  #ggsave(paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Plots/\",hist_pool_date,\".png\"),\n         #flood_dur_plot,width = 8, height = 6,dpi = 300)\n}\n\n# Convert the results to a usable df\nthur_dur &lt;- do.call(rbind.data.frame,thur_flood_durations)\n\nthur_dur contains the inflow duration of each high pool event. This was plotted as a histogram with a binwidth of 1-day to determine an initial estimate for the critical inflow duration.\n\nmdur &lt;- mean(thur_dur$Dur_days)\nsddur &lt;- sd(thur_dur$Dur_days)\nmeddur &lt;- median(thur_dur$Dur_days)\n\n# Base R does not have a Mode function.\nMode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Compute the mode\nprint(paste0(\"The mode is: \",Mode(thur_dur$Dur_days),\" days\"))\n\n[1] \"The mode is: 4 days\"\n\n\nThe resulting hyetograph is shown below.\n\n# setting up a ggplot looks worse than it is\nggplot(data = thur_dur,aes(x=Dur_days))+\n  geom_histogram(binwidth=1,fill=histcolor,color=linecolor,alpha=0.9)+\n  scale_x_continuous(breaks = seq(1,14,1))+\n  theme_USACE()+\n  labs(x = \"Flood Duration (Days)\",y=\"Count\",\n       subtitle = \"Based on AMS Flood Events (1962 - 2024)\")+\n  ggtitle(\"Distribution of Flood Durations at Thurmond Dam\")+\n  geom_vline(xintercept = mdur, color=meancolor)+\n  geom_vline(xintercept = (mdur+sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_vline(xintercept = (mdur-sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_text(aes(x=mdur, label=paste(\"\\nMean = \",round(mdur,2),sep=\"\"),\n                y=7.5), colour=meancolor, angle=90)+\n  geom_text(aes(x=(mdur+sddur), label=paste(\"\\n+1SD = \",round(mdur+sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  geom_text(aes(x=(mdur-sddur), label=paste(\"\\n-1SD = \",round(mdur-sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nBased on the results of the AMS duration distribution, the critical inflow duration is likely 3-, 4-, or 5-days. With this in mind, five historical peak reservoir events were identified to estimate a final critical duration. The loop used to create the inflow duration distribution contained a code to export a csv and png of each AMS event. These were used to estimate a final critical inflow duration. This has been replicated below.\n\nevent &lt;- c(\"11-Apr-1964\",\"30-Mar-1964\",\"3-Mar-1971\",\"17-Mar-1975\",\"7-Feb-1998\",\"1-Jan-2016\")\nduration &lt;- c(5,4,4,4.5,4.5,4)\n\nmean_inflow_dur &lt;- round(mean(duration),2)\ngeomean_inflow_dur &lt;- round(exp(mean(log(duration))),2)\n\ncat(\"The mean inflow duration is \",mean_inflow_dur,\"\\nThe geometric mean inflow duration is \", geomean_inflow_dur)\n\nThe mean inflow duration is  4.33 \nThe geometric mean inflow duration is  4.32\n\n\nBoth 4-day and 5-day could be considered for the critical inflow duration. During the project, the 4-day inflow was selected as the critical duration.\nAs a preemptive analysis of seasonallity, the stage seasonallity was analyzed.\n\nggplot(data = annual_max_stage88,aes(x = Month))+\n  geom_histogram(binwidth=1,fill=\"lightblue\",color=\"black\",alpha=0.9)+\n  stat_density(aes(y = ..count..), geom = \"line\", color = \"green3\", size = 1) +\n  scale_x_continuous(breaks = seq(1,12,1),\n                     labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"June\",\"July\",\"Aug\",\"Sept\",\"Oct\",\"Nov\",\"Dec\"))+\n  scale_y_continuous(breaks = seq(0,20,2),minor_breaks = seq(0,20,1))+\n  theme_USACE()+\n  labs(x = \"Month\",y=\"Count\",subtitle = \"Based on AMS of POR Stage Elev.(1962 - 2024)\")+\n  ggtitle(\"Flood Seasonality at Thurmond Dam\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe next steps were to create a timeseries of unregulated inflow data that can be used to create an AMS of 4-day inflow volumes. This section incorporates code used in CreatingUnregulatedData_Augusta.R and Crtical Volume Duration.R.\nThe Augusta USGS Streamgage was the subject of a 1990 USACE & USGS report that analyzed the relationship between regulated and unregulated flow-frequency estimates. Due to the existence of this study and the longer gage record, the Augusta gage was used as the primary data source for inflow data. The overlapping record of the Augusta gage with the former USGS streamgage at JST Dam was used to verify the drainage area ratio from the design memo (0.93).\n\\[ Q_T = 0.93 * Q_A\\]\nIn order to create a 4-day inflow volume annual maximum series (AMS), the daily streamflow data at the Augusta gage was processed using the following steps:\n\nThe effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\nDaily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in Figure 14 (PA Report - Chapter 4). This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n4-day unregulated volume AMS was obtained for each water year\nThe peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\nThe peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\nThe drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\nThese steps & figures will be replicated below.\n\n\nBefore any of the steps above, the data should be imported and formatted.\n\n# Import Augusta Daily & Format Dates\nlibrary(\"dataRetrieval\")\n\n# Import using Augusta Site Number\nsiteNo &lt;- \"02197000\"\npCode &lt;- \"00060\" #00061 - # Check codes from USGS\nstart.date &lt;- \"1700-01-01\" # arbitraty to get all info\nend.date &lt;- \"2024-12-31\"\n\naugusta_daily &lt;- readNWISdv(siteNumbers = siteNo,\n                            parameterCd = pCode,\n                            startDate = start.date,\n                            endDate = end.date)\n\naugusta_daily &lt;- renameNWISColumns(augusta_daily)\n\naugusta_daily &lt;- augusta_daily %&gt;% mutate(DT = as.Date(Date),.before = Flow)\n\n# Add water year column\naugusta_daily &lt;- augusta_daily %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_daily &lt;- augusta_daily %&gt;% mutate(WaterYear = ifelse(Mon &gt; 9, Yr + 1, Yr))\n\n# Make sure the data is there\nggplot(data = augusta_daily) + geom_line(aes(x=DT,y=Flow)) + theme_bw()\n\n\n\n\n\n\n\n\nThe regulated-unregulated relationship was digitized from the 1990 report and imported into R.\n\n# Import Digitized Data for Figure 35 (1990) - Exceedance Prob for Unregulated and Regulated\nfig35 &lt;- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Figure35data.csv\")\nfig35 &lt;- fig35 %&gt;% mutate(NEP = 100-ExceedanceProb)\n\n# Convert Reg and Unreg at Augusta\naugusta_conversion &lt;- fig35 %&gt;% mutate(Aug_RegQ = PeakQRegulated,Aug_UnregQ = PeakQUnregulated)\naugusta_conversion &lt;- augusta_conversion %&gt;% select(c(Aug_RegQ,Aug_UnregQ))\n\n# Recreate Figure 35\nlogscalebreaks = c(1000,5000,10000,20000,50000,100000,200000,500000,1000000)\nlogscaleminor_breaks = c(seq(2000, 9000, by = 1000),seq(20000, 90000, by = 10000),\n                         seq(200000, 900000, by = 100000))\n\nggplot(data=fig35,aes(x=PeakQUnregulated,y=PeakQRegulated))+\n  geom_point()+\n  geom_line()+\n  theme_USACE()+\n  ggtitle(\"Savannah River at Augusta, GA (02197000)\")+\n  labs(x=\"Unregulated Discharge (cfs)\",y=\"Regulated Discharge (cfs)\",subtitle = \"Relation between regulated peak discharges and unregulated peak discharges\",\n       color = \"Legend\")+\n  theme(plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5),legend.position = \"bottom\")+\n  geom_abline(intercept = 0, slope = 1, color = \"blue1\", linetype = \"dashed\") +\n  scale_y_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  scale_x_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  coord_cartesian(xlim = c(5000,1000000),ylim = c(5000,1000000))\n\n\n\n\n\n\n\n\nThe Peak Flow dataset at Augusta will also be used. This data was obtained from the USGS website. Additional post processing was done to make this easier (i.e. some manual entry to Flow Notes)\n\n# Import and Format\naugusta_peak_ams &lt;- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Augusta Gage/csv/Augusta_Peak_AMS.csv\",header = T)\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% mutate(DT = as.Date(Date),.before = Flow)\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% mutate(WaterYear = ifelse(Mon &gt; 9, Yr + 1, Yr))\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% rename(Peak_Flow = Flow)\n\n\nThe effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\n\n\n# Regulation Assumed to start in 1951\nregulation_wy &lt;- 1951\npre_reg &lt;- augusta_daily %&gt;% filter(WaterYear &lt; regulation_wy)\naug_reg &lt;- augusta_daily %&gt;% filter(WaterYear &gt;= regulation_wy)\n\npre_reg_peak &lt;- augusta_peak_ams %&gt;% filter(WaterYear &lt; regulation_wy)\naug_reg_peak &lt;- augusta_peak_ams %&gt;% filter(WaterYear &gt;= regulation_wy)\n\n\nDaily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in the relationship above. This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n\n\n# Estimate Unregulated from approx function\nAugusta_Unreg &lt;- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ, xout = aug_reg$Flow)[2]\nAugusta_Unreg_peak &lt;- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ,\n                             xout=aug_reg_peak$Peak_Flow)[2]\n\n# add to regulated data\naug_reg &lt;- aug_reg %&gt;% mutate(Unregulated_Flow = round(Augusta_Unreg$y,0))\naug_reg_peak &lt;- aug_reg_peak %&gt;% mutate(Unregulated_PeakFlow = round(Augusta_Unreg_peak$y,0))\n\n# create a 1:1 column of unregulated flow to make bind_rows easier\npre_reg &lt;- pre_reg %&gt;% mutate(Unregulated_Flow = round(Flow,0))\npre_reg_peak &lt;- pre_reg_peak %&gt;% mutate(Unregulated_PeakFlow = round(Peak_Flow,0))\n\n# Combine datasets\naugusta_daily_unreg &lt;- bind_rows(pre_reg,aug_reg)\nAMS_Peak &lt;- bind_rows(pre_reg_peak,aug_reg_peak)\n\n# remove any confusion\nrm(Augusta_Unreg)\nrm(Augusta_Unreg_peak)\n\n# Check results for sanity \nggplot(data = augusta_daily_unreg) +\n  geom_line(aes(x=DT,y=Unregulated_Flow,color = \"Unregulated\"))+\n  geom_line(aes(x=DT,y=Flow,color=\"Existing\"))+\n  theme_USACE()+\n  labs(x = \"Date\",\n       y = \"Daily Flow (cfs)\",\n       title = \"Daily Flow Values Augusta (02197000)\",\n       subtitle = \"Post-WY 1951 Converted to Unregulated\")+\n  scale_y_continuous(breaks = seq(0,250000,50000),minor_breaks = seq(0,250000,10000))+\n  scale_x_date(breaks = \"10 year\",minor_breaks = \"1 year\")+\n  scale_color_manual(values = c(\"#377EB8\", \"red2\"), \n                     name = \"Legend\", \n                     labels = c(\"Existing\",\"Unregulated\"))+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),legend.position = c(.85,.8))\n\n\n\n\n\n\n\n\n\n4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n\n\n# Create Data with gaps to avoid rolling mean over the gaps (i.e. 1891-12-31 & 1896-01-01)\ncomplete_dates &lt;- seq(min(augusta_daily_unreg$DT), max(augusta_daily_unreg$Date), by = \"day\")\naug_unreg_complete &lt;- merge(augusta_daily_unreg, data.frame(Date = complete_dates), by = \"Date\", all = TRUE)\n# aug_unreg_complete[3010:3020,]  # check\n#naniar::vis_miss(aug_unreg_complete) # see gaps in data, uncomment to use\n\ncritdur &lt;- 4\n# rollmean is a 4 day rolling average. rollmeanr indicates that the window is aligned to the \"right\"\n# This will compute a time series of 4-day inflow volumes\naug_unreg_complete &lt;- aug_unreg_complete %&gt;% mutate(VolDur_4day = round(rollmeanr(Unregulated_Flow,k = critdur, fill=NA),0))\n\n\n4-day unregulated volume AMS was obtained for each water year\n\n\nAMS_4day &lt;- aug_unreg_complete %&gt;%\n  # Group by WY\n  group_by(WaterYear) %&gt;%\n  # Take the largest value from each WY\n  top_n(1, VolDur_4day) %&gt;%\n  # Ensures no duplicate years\n  distinct(WaterYear, .keep_all = TRUE) %&gt;%\n  ungroup()\n\n# Combine Peak and 4-day AMS to help stay organized\nall_AMS &lt;- left_join(AMS_Peak,AMS_4day,by = \"WaterYear\")\ncolnames(all_AMS)\n\n [1] \"Agency\"               \"Site\"                 \"Date.x\"              \n [4] \"DT.x\"                 \"Peak_Flow\"            \"Flow_notes\"          \n [7] \"Gage\"                 \"Gage_notes\"           \"Yr.x\"                \n[10] \"Mon.x\"                \"Day.x\"                \"WaterYear\"           \n[13] \"Unregulated_PeakFlow\" \"Date.y\"               \"agency_cd\"           \n[16] \"site_no\"              \"DT.y\"                 \"Flow\"                \n[19] \"Flow_cd\"              \"Yr.y\"                 \"Mon.y\"               \n[22] \"Day.y\"                \"Unregulated_Flow\"     \"VolDur_4day\"         \n\nall_AMS &lt;- all_AMS %&gt;% select(c(DT.x,Peak_Flow,Flow_notes,WaterYear,Unregulated_PeakFlow,DT.y,Unregulated_Flow,VolDur_4day))\nall_AMS &lt;- all_AMS %&gt;% rename(DT_Peak = DT.x, DT_Daily = DT.y)\n\n\nThe peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\n\n\n# Find the peaks and 4-day vols that are from the same event. 4 is the search threshold then\nmatchingAMS &lt;- all_AMS %&gt;% filter(DT_Peak &gt;= DT_Daily - 4 | DT_Peak &lt;= DT_Daily + 4)\nmatchingAMS &lt;- matchingAMS %&gt;% mutate(PeaktoVol = Unregulated_PeakFlow/VolDur_4day)\n\n# Now Add this peak to vol data back to the AMS dataset\nall_AMS &lt;- all_AMS %&gt;% mutate(Ratio_Peak_4d = ifelse(DT_Peak &gt;= DT_Daily - 4 | DT_Peak &lt;= DT_Daily + 4,Unregulated_PeakFlow/VolDur_4day,NA))\n\ncat(\"The mean peak-to-volume ratio is \",round(mean(matchingAMS$PeaktoVol),2),\"\\nThe geometric mean peak-to-volume ratio is \", round(exp(mean(log(matchingAMS$PeaktoVol))),2),\"\\nThe median peak-to-volume ratio is \",round(median(matchingAMS$PeaktoVol),2))\n\nThe mean peak-to-volume ratio is  1.54 \nThe geometric mean peak-to-volume ratio is  1.46 \nThe median peak-to-volume ratio is  1.39\n\n\n\nggplot(data = matchingAMS)+\n  geom_histogram(aes(x =PeaktoVol),fill=\"lightblue\",color=\"black\",alpha=0.9,\n                 breaks = seq(0.9,3.0,0.1))+\n  theme_USACE()+\n  ggtitle(\"Ratio of Peak Flow to 4-Day Volume at Augusta (02197000)\")+\n  labs(x=\"Peak Flow/4-Day Vol\", y=\"Count\")+\n  scale_x_continuous(breaks = seq(0.9,3.0,0.1),minor_breaks = seq(0,3.0,0.05))+\n  scale_y_continuous(breaks = seq(0,25,1))+\n  geom_vline(xintercept = mean(matchingAMS$PeaktoVol), color=\"orange2\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+\n  theme(axis.text.x = element_text(size=6.5))\n\n\n\n\n\n\n\n\n\nThe peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\n\n\n# Peak to volume ratio\nPtVratio_4day &lt;- mean(matchingAMS$PeaktoVol)\n\n# Calculate missing 4 day volumes\nall_AMS &lt;- all_AMS %&gt;% mutate(Peak_to_4day = round(Unregulated_PeakFlow/PtVratio_4day,0))\n\nall_AMS &lt;- all_AMS %&gt;% mutate(Complete_4Day_AMS = ifelse(is.na(all_AMS$VolDur_4day),Peak_to_4day,VolDur_4day))\n\n\nThe drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\n\n# Previously calculated drainage area ratio\nDAratio &lt;- 0.93\n\n# Multiply the Augusta AMS values by the DA ratio\nall_AMS &lt;- all_AMS %&gt;% mutate(Thurm_AMS_4day = round(DAratio*Complete_4Day_AMS,0))\n\nBONUS STEP: Prepare the data to be used in BestFit. This will use the flow notes to set perception threshold values & interval values (assumed to be +/- 20%). The interval calculation can be tough to decipher via code. It has been written out as an equation below.\n\\[ Interval_L = 0.80 * 0.93 * PeakQ/PeakToVol\\] \\[ Interval_U = 1.20 * 0.93 * PeakQ/PeakToVol\\]\n\n# Rename some columns to make it easier - this is because my variable naming was not as intuitive as it should have been\nall_AMS &lt;- all_AMS %&gt;% rename(Aug_AMS_1day = Unregulated_Flow, \n                              Aug_AMS_4day = VolDur_4day,\n                              Aug_Complete_4Day_AMS = Complete_4Day_AMS)\n\n# If there is no peak to 4day ratio, this was historic peak flow information\nall_AMS &lt;- all_AMS %&gt;% mutate(BestFitNotes = ifelse(is.na(Ratio_Peak_4d),\"Estimated from Peak AMS\",NA))\n\n# Computing intervals. ifelse ensures it only computs missing ones\nall_AMS &lt;- all_AMS %&gt;% mutate(Interval_Lower =\n                                ifelse(WaterYear&lt;1876,round((DAratio*0.8*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_MostLikely =                                 \n                                ifelse(WaterYear&lt;1876,round((DAratio*1.0*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_Upper =\n                                ifelse(WaterYear&lt;1876,round((DAratio*1.2*Unregulated_PeakFlow)/PtVratio_4day,0),NA))\n\nThe allAMS dataframe can be exported to .csv or it can be further filtered to only include fields required for BestFit.\n\n# Exporting allAMS - commented out\n# write.csv(all_AMS,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_AMS_Peak_1day_4day.csv\",row.names = F)\n\n# Selecting fields from allAMS that are required for BestFit\nbestfit_input &lt;- all_AMS %&gt;% select(c(WaterYear,Peak_Flow,Flow_notes,Unregulated_PeakFlow,\n                                      Thurm_AMS_4day,BestFitNotes,\n                                      Interval_Lower,Interval_MostLikely,Interval_Upper))\n\n# Export BestFit to csv - commented out\n#write.csv(bestfit_input,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_BestFitINPUT.csv\",row.names = F)\n\n\n\n\n\nMove this before the other code blocks for ggplot themes. If issues continue, remove +theme_USACE() from the ggplot chunks\n\ntheme_USACE &lt;-  function(base_size = 8){theme(\n  text = element_text(family = 'serif', color = 'black'),\n  line = element_line(colour = 'black', linewidth = 0.2), \n  rect = element_rect(colour = 'black', linewidth = 0.2),\n  plot.title = element_text(vjust = 3, size = 9),\n  plot.margin = unit(c(1,1,1,1), 'lines'),\n  panel.border = element_rect(fill = F),\n  panel.grid.major = element_line(colour = 'grey50', linewidth = 0.2),\n  panel.grid.minor = element_line(colour = 'grey75', linewidth = 0.1),\n  panel.background = element_rect(fill = 'white'),\n  #defaults legend to upper left, can/should be overridden based on graph\n  #legend.background = element_blank(),\n  legend.background = element_rect(fill = \"lightgrey\", colour = \"black\"),\n  legend.justification = c(\"left\", \"top\"),\n  legend.position = c(0.8, 0.5),\n  # this value should be adjusted dependent on \n  # graph with the addition of another \n  # theme(legend.position = c(X, Y)) argument after theme_USACE()\n  # or... for no legend \n  # legend.position = element_blank(),\n  legend.key = element_blank(),\n  legend.title = element_text(size = 9),\n  #legend.title = element_blank(), \n  axis.title.x = element_text(size = 9),\n  axis.title.y = element_text(angle = 90, size = 9),\n  axis.text.x = element_text(margin = margin(8, 0, 0, 0)),\n  axis.text.y = element_text(margin = margin(0, 8, 0, 0)),\n  axis.ticks.length = unit(0.25 , 'cm')\n)}"
  },
  {
    "objectID": "JST_inR.html#period-of-record-data-analysis",
    "href": "JST_inR.html#period-of-record-data-analysis",
    "title": "Thurmond PA - R Scripting",
    "section": "",
    "text": "Data requests for existing period of record (POR) data should be completed through the home district’s water management section. Typical data stored for projects throughout the USACE portfolio range from observed lake elevation, stage, flow, precipitation, temperature, snow depth, snow water equivalent, and computed inflow at various time intervals.\nThe POR data for JST Dam was obtained from the (internal facing) Savannah District Water Management website. The data was copy-pasted from the historic data query into Notepad++ and saved as .txt file. The data needs to be delimited prior to use in R.\n\n\nThe following code blocks were taken from ThurmondPOR_Data.R. The first block below also serves as a format for scripting best practices. Commenting code is invaluable (comments are the only way I can remember what I did in the script, months later).\nThis section contains a commented header (note: # Header ##### will name a section of code). It is good practice to clear the environmental at the start of a script. The necessary libraries are also called at the top of a script. In order to load a library, it has to have been previously downloaded. install.packages has been included and commented out as an example. The line theme_USACE.r is ggplot theme created by Brian Breaker.\n\n# Thurmond POR data ############################################################\n# Inputs are raw text files copied from WM Site\n# Dan McGraw\n# 6-March-2024\n################################################################################\n\n## Clear workspace\nrm(list = ls(all.names = TRUE))\n\n## Load Libraries\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nsource(\"E:/R/theme_USACE.r\")\n\nThe next step is to read the delimited test files containing the POR data. These are all saved in the same folder which is pordir. Note that the data should be in .csv or .txt files.\n\n# Read files ###################################################################\n# Locate directory/folder with POR data\npordir &lt;- \"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/\"\n\n# Create file path of the POR txt files\nthur_elev_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_ELEV.txt\")\nthur_flows_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_NetInflow_Discharge.txt\")\nthur_stor_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_STOR.txt\")\nthur_gen_file &lt;- paste0(pordir,\"ThurmondDL_3-6-2024_GEN.txt\")\n\n# Read text files\nthur_elev &lt;- read.table(thur_elev_file)\nthur_flows &lt;- read.table(thur_flows_file)\nthur_stor &lt;- read.table(thur_stor_file)\nthur_gen &lt;- read.table(thur_gen_file)\n\nHeadings were not read in with the data. These were added after import based on existing user knowledge of the fields. This code block is using dplyr from the tidyverse package. Tidyverse works very well with the pipe operator (%&gt;%). Think of this as:\nnew data name &lt;- old data name &gt;input&gt; function\n\nthur_elev &lt;- thur_elev %&gt;% rename(Date = V1,DOW = V2, Elev_29 = V3) \nthur_flows &lt;- thur_flows %&gt;% rename(Date = V1,DOW = V2, NetInflow = V3, Discharge = V4) \nthur_stor &lt;- thur_stor %&gt;% rename(Date = V1,DOW = V2, Storage = V3) \nthur_gen &lt;- thur_gen %&gt;% rename(Date = V1,DOW = V2, Gen = V3)\n\nAn example of the thur_flows dataset is below. Note the data was downloaded in descending date, this will be important later.\n\nhead(thur_flows)\n\n        Date DOW NetInflow Discharge\n1 03/05/2024 Tue     10412      6177\n2 03/04/2024 Mon     15313      6490\n3 03/03/2024 Sun      6656      6656\n4 03/02/2024 Sat      6341      6694\n5 03/01/2024 Fri      7259      4436\n6 02/29/2024 Thu      2606      4371\n\n\nThe day of the week was not significant for this analysis. It was dropped from the dataset.\n\nthur_elev &lt;- thur_elev %&gt;% select (-c(DOW))\nthur_flows &lt;- thur_flows %&gt;% select (-c(DOW))\nthur_stor &lt;- thur_stor %&gt;% select (-c(DOW))\n\nThe date field will be very important for the analysis. Initially, it is common for R to read the date field as a character data-type. This means there is no numerical basis to the values (the dates are just words). The dates need to be converted a date data-type for this analysis. For example, once the dates are formatted correctly, they can be arranged by increasing date. This is done using the as.Date function. Errors can arise when the dates are not formatted uniformly. The function allows several “try” inputs for cases like that. Another breakdown of the pipe operator is shown above the code block:\nthur_flows_updated = thur_flows_existing + a new DATE column (mutate), called DT, that will go before NetInflow\n\nthur_flows &lt;- thur_flows %&gt;% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = NetInflow )\nthur_elev &lt;- thur_elev %&gt;% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Elev_29)\nthur_stor &lt;- thur_stor %&gt;% mutate(DT = as.Date(Date,format = \"%m/%d/%Y\"),.before = Storage)\n\nSee the difference:\n\nclass(thur_flows$Date)\n\n[1] \"character\"\n\nclass(thur_flows$DT)\n\n[1] \"Date\"\n\n\nNow the dates can be arranged by increasing date.\n\nthur_elev &lt;- thur_elev %&gt;% arrange(DT)\nthur_flows &lt;- thur_flows %&gt;% arrange(DT)\nthur_stor &lt;- thur_stor %&gt;% arrange(DT)\n\nAn advantage of using R instead of excel is the ability to do column-wide functions without click & drag. The elevation data needs to be in NAVD88. The conversion from NGVD29 to NAVD88 is: \\[ NAVD88 = NGVD29 - 0.7\\]\n\n# Create a new dataframe (df) for NAVD88 elevation\nthur_elev88 &lt;- thur_elev %&gt;% mutate(Elev_88 = Elev_29 - 0.7)\n\n# Drop the ngvd29 column from the navd88 df to avoid confusion\nthur_elev88 &lt;- thur_elev88 %&gt;% select(-c(Elev_29))\n\nAt this point of the analysis the data was exported as csv files. These csv files could serve as a starting point for analysis in different scripts, without having to clean up the data as much (date will always need to be formatted as.Date). Note that an R & DSS package exists: https://github.com/eheisman/dssrip. The write.csv function has been commented out so the code will not export any files during this walkthrough.\n\nthur_elev_outfile &lt;- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV.csv\")\nthur_elev88_outfile &lt;- paste0(pordir,\"csv/Thurmond3-6-2024_ELEV_88.csv\")\nthur_flows_outfile &lt;- paste0(pordir,\"csv/Thurmond_3-6-2024_NetInflow_Discharge.csv\")\nthur_stor_outfile &lt;- paste0(pordir,\"csv/Thurmond_3-6-2024_STOR.csv\")\n\n#write.csv(thur_elev,thur_elev_outfile)\n#write.csv(thur_elev88,thur_elev88_outfile)\n#write.csv(thur_flows,thur_flows_outfile)\n#write.csv(thur_stor,thur_stor_outfile)\n\n\n\n\nAn important first step of data analysis is understanding what the data looks like. R was used to quickly provide historic pool levels, large inflows, and additional time series context.\n\n# What years are contained within the POR\nthryears &lt;- unique(year(thur_flows$DT))\n\n# What does the POR data look like\nggplot(data = thur_flows,aes(x=DT,y=Discharge)) + geom_point()+theme_USACE()\n\n\n\n\n\n\n\n\nHow many inflows above 80,000 cfs (arbitrary) have there been?\n\nthur_flows %&gt;% filter(NetInflow &gt; 80000)%&gt;%\n  ggplot(aes(x=DT,y=NetInflow))+geom_point() + theme_USACE()\n\n\n\n\n\n\n\n\nThe top 10 largest pool elevations and inflows can be obtained as follows:\n\n# MAX Pool Elevations (top 10)\nthur_elev %&gt;% top_n(10,Elev_29)%&gt;% arrange(desc(Elev_29))\n\n         Date         DT Elev_29\n1  04/11/1964 1964-04-11  336.46\n2  04/10/1964 1964-04-10  336.31\n3  12/31/2015 2015-12-31  335.94\n4  01/01/2016 2016-01-01  335.94\n5  01/02/2016 2016-01-02  335.93\n6  04/09/1964 1964-04-09  335.73\n7  02/07/1998 1998-02-07  335.45\n8  05/06/1964 1964-05-06  335.40\n9  03/30/1964 1964-03-30  335.38\n10 02/06/1998 1998-02-06  335.37\n\n\n\n# MAX Pool Elevations (top 10)\nthur_flows %&gt;% top_n(10,NetInflow) %&gt;% arrange(desc(NetInflow))\n\n         Date         DT NetInflow Discharge\n1  03/03/1971 1971-03-03    116127      3634\n2  03/14/1975 1975-03-14    108026     14826\n3  04/08/1964 1964-04-08    104761     36554\n4  03/26/1964 1964-03-26    100266     14519\n5  02/04/1998 1998-02-04     99121     16127\n6  04/07/1964 1964-04-07     97413     12778\n7  03/27/1964 1964-03-27     92612     15964\n8  04/09/1964 1964-04-09     91136     70782\n9  01/26/1978 1978-01-26     90983     15167\n10 04/16/1969 1969-04-16     87295      6716\n\n\n\n\n\nHydrologic analysis uses annual maxima series (AMS) based on water years (starting on October 1). Using the correctly formatted date column, an AMS can be created for inflow and reservoir pool elevation/stage. Note that the reservoir inflow is regulated. The first step is determining the water year of each data point.\n\n# Create separate columns of Year, Month, Day as numbers\nthur_flows &lt;- thur_flows %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\n\n# ifelse is a binary yes/no operator. If the month greater than 10, WY = Yr+1\nthur_flows &lt;- thur_flows %&gt;% mutate(WaterYear = ifelse(Mon &gt;= 10, Yr + 1, Yr))\n\nthur_elev88 &lt;- thur_elev88 %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\nthur_elev88 &lt;- thur_elev88 %&gt;% mutate(WaterYear = ifelse(Mon &gt;= 10, Yr + 1, Yr))\n\nNow that the data can be grouped by water year, the annual max of each water year can be obtained\n\nannual_max_flows &lt;- thur_flows %&gt;%\n  group_by(WaterYear) %&gt;%\n  summarize(MaxInflow = max(NetInflow),\n            Date = DT[which.max(NetInflow)],\n            Month = month(DT[which.max(NetInflow)]),\n            Day = day(DT[which.max(NetInflow)]))\n\nannual_max_stage88 &lt;- thur_elev88 %&gt;%\n  group_by(WaterYear) %&gt;%\n  summarize(Max_Elev = max(Elev_88),\n            Date = DT[which.max(Elev_88)],\n            Month = month(DT[which.max(Elev_88)]),\n            Day = day(DT[which.max(Elev_88)]))\n\n# export if necessary\n# write.csv(annual_max_stage88,paste0(pordir,\"Stage_AMS.csv\"))\n# write.csv(annual_max_flows,paste0(pordir,\"Reg_Flow_AMS.csv\"))\n\n\n\n\nThis section of code aimed to determine the distribution of inflow durations at JST Dam. The procedure used was from from RMC-TR-2018-03 - “Hydrologic Hazard Methodology for Semi-Quantitative Risk Assessments”. The TR defines critical inflow duration as the inflow duration that results in the highest water surface elevations for the reservoir of interest.\nAccording to RMC-TR-2018-03, three to 5 (3-5) historical peak reservoir events should be identified. The inflow, discharge, and stage for these events should be used to determine the critical inflow duration. The TR states “select events that are consistent with the types of events likely to be the driver of extreme peak stages”.\nDue to the highly regulated nature of JST dam, the driver of extreme stage could vary from shorter, localized inflows to the dam OR longer, basin-wide inflows to the dam. In order to reach an understanding of basin operations as well as a good starting point for the critical inflow duration, the following analyses were done:\n\n\n\nDetermine the daily rate of change of reservoir elevation (ft/day)\nIdentify the top 150 daily rates (increasing)\n\n\n# Create and Set WSE Rate column to 0\nthur_elev88$WSERate &lt;- NA\nthur_elev88$WSERate[1]&lt;-0\n\n# Get rate of changer per day\nfor (i in 2:length(thur_elev88$Elev_88)){\n  day2 = thur_elev88$Elev_88[i]\n  day1 = thur_elev88$Elev_88[i-1]\n  stagedelta = day2 - day1\n  thur_elev88$WSERate[i] = stagedelta\n}\n\n# Select the top 150 rates\nFastRise &lt;- thur_elev88 %&gt;%\n  arrange(desc(WSERate)) %&gt;% slice_max(WSERate,n=150)\n\n# Plot the rates against their corresponding reservoir elevations\nggplot(data=FastRise,aes(x=WSERate,y=Elev_88))+geom_point()+theme_USACE()+\n  labs(x = \"Max Daily Rate of Change (ft/day)\", y = \"Peak Reservoir Elev (NAVD88)\")\n\n\n\n\n\n\n\n\nThis is helpful, but it can only tell so much since the inflow data is daily.\n\n\n\n\nLoop through all high pool events and obtain corresponding inflow and discharge hydrographs from 7 days before and after\nFind days during event where inflow &gt; discharge\nSave the hydrograph image and table\nCreate distribution of inflow durations\n\nThis code will save a csv and png of the inflow event\n\n# NOTE annual_max_stage88$Date is a formatted date column\nams_dates &lt;- annual_max_stage88$Date\n# ams_dates &lt;- annual_max_flows$Date\n\neventDTs &lt;- ams_dates\n\n# Grab flow hydrographs 14 days before and after max pool\neventDTs_start &lt;- eventDTs - 7\neventDTs_end &lt;- eventDTs + 7\n\nthur_rep_hydrographs &lt;- list()\nthur_flood_durations &lt;- list()\nflow.colors &lt;- c(\"Inflow\" = \"#333BFF\", \"Outflow\" = \"orangered2\")\n\nfor (i in 1:length(eventDTs)){\n  hist_pool_date &lt;- eventDTs[i]\n  \n  dt1 &lt;- which(thur_flows$DT == eventDTs_start[i])\n  dt2 &lt;- which(thur_flows$DT == eventDTs_end[i])\n  \n  if (eventDTs_start[i] &lt; thur_flows$DT[1]){\n    dt1 &lt;- 1\n  }\n  \n  hydrograph &lt;- thur_flows[dt1:dt2,]\n  hydrograph$Event &lt;- eventDTs[i]\n  thur_rep_hydrographs[[i]] &lt;- hydrograph\n  \n  plot_date &lt;- as.character(format(hist_pool_date, \"%d-%b-%Y\"))\n  flood_dur_plot&lt;-ggplot(data = hydrograph)+\n    geom_line(aes(x=DT,y=NetInflow,color=\"Inflow\"))+geom_point(aes(x=DT,y=NetInflow,color=\"Inflow\"))+\n    geom_line(aes(x=DT,y=Discharge,color=\"Outflow\"))+geom_point(aes(x=DT,y=Discharge,color=\"Outflow\"))+\n    theme_USACE()+\n    ggtitle(paste0(\"Record Pool Event on \",plot_date))+\n    scale_x_date(date_breaks = \"1 day\")+labs(x=\"Date\",y=\"Discharge(cfs)\",color = \"Legend\")+\n    scale_color_manual(values = flow.colors)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n    theme(plot.title = element_text(hjust = 0.5))+\n    theme(legend.position = c(0.8,0.8))\n  \n  # Find where Inflow &gt; outflow for flood duration\n  flood_dates &lt;- hydrograph$DT[hydrograph$NetInflow &gt;= hydrograph$Discharge]\n  peakflowdate &lt;- hydrograph$DT[max(hydrograph$NetInflow) == hydrograph$NetInflow]\n  event_index &lt;- which(flood_dates == peakflowdate)\n  \n  if (length(event_index) == 0){\n    event_index &lt;- length(flood_dates)+3\n  }\n  \n  # Find consecutive dates around eventDT\n  start_index &lt;- event_index\n  end_index &lt;- event_index\n  \n  # Expand the start index to include previous consecutive dates\n  while (start_index &gt; 1 && flood_dates[start_index] - flood_dates[start_index - 1] == 1) {\n    start_index &lt;- start_index - 1\n  }\n  \n  # Expand the end index to include following consecutive dates\n  while (end_index &lt; length(flood_dates) && flood_dates[end_index + 1] - flood_dates[end_index] == 1) {\n    end_index &lt;- end_index + 1\n  }\n  \n  # Extract consecutive dates\n  consecutive_dates &lt;- flood_dates[start_index:end_index]\n  \n  # Flood Duration in Days\n  flood_dur &lt;- length(consecutive_dates)\n  \n  # march 21 2006 is tough, duration likely = 4-5 but outflow was always larger than inflow\n  # peak outflow matched peak inflow\n  \n  thur_flood_durations[[i]] &lt;- data.frame(Event = hist_pool_date,Dur_days = flood_dur)\n\n  # Write CSV - uncomment to save\n  #write.csv(hydrograph,paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Flood_\",hist_pool_date,\".csv\"),row.names = F)\n  \n  # Save plot of flood event - uncomment to save \n  #ggsave(paste0(\"E:/1.Thurmond/Chapter 4/HH/Data/Project POR Data/FloodDurations/Plots/\",hist_pool_date,\".png\"),\n         #flood_dur_plot,width = 8, height = 6,dpi = 300)\n}\n\n# Convert the results to a usable df\nthur_dur &lt;- do.call(rbind.data.frame,thur_flood_durations)\n\nthur_dur contains the inflow duration of each high pool event. This was plotted as a histogram with a binwidth of 1-day to determine an initial estimate for the critical inflow duration.\n\nmdur &lt;- mean(thur_dur$Dur_days)\nsddur &lt;- sd(thur_dur$Dur_days)\nmeddur &lt;- median(thur_dur$Dur_days)\n\n# Base R does not have a Mode function.\nMode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Compute the mode\nprint(paste0(\"The mode is: \",Mode(thur_dur$Dur_days),\" days\"))\n\n[1] \"The mode is: 4 days\"\n\n\nThe resulting hyetograph is shown below.\n\n# setting up a ggplot looks worse than it is\nggplot(data = thur_dur,aes(x=Dur_days))+\n  geom_histogram(binwidth=1,fill=histcolor,color=linecolor,alpha=0.9)+\n  scale_x_continuous(breaks = seq(1,14,1))+\n  theme_USACE()+\n  labs(x = \"Flood Duration (Days)\",y=\"Count\",\n       subtitle = \"Based on AMS Flood Events (1962 - 2024)\")+\n  ggtitle(\"Distribution of Flood Durations at Thurmond Dam\")+\n  geom_vline(xintercept = mdur, color=meancolor)+\n  geom_vline(xintercept = (mdur+sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_vline(xintercept = (mdur-sddur), color=sdcolor, linetype = \"dashed\")+\n  geom_text(aes(x=mdur, label=paste(\"\\nMean = \",round(mdur,2),sep=\"\"),\n                y=7.5), colour=meancolor, angle=90)+\n  geom_text(aes(x=(mdur+sddur), label=paste(\"\\n+1SD = \",round(mdur+sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  geom_text(aes(x=(mdur-sddur), label=paste(\"\\n-1SD = \",round(mdur-sddur,2),sep=\"\"), \n                y=7.5), colour=sdcolor, angle=90)+\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nBased on the results of the AMS duration distribution, the critical inflow duration is likely 3-, 4-, or 5-days. With this in mind, five historical peak reservoir events were identified to estimate a final critical duration. The loop used to create the inflow duration distribution contained a code to export a csv and png of each AMS event. These were used to estimate a final critical inflow duration. This has been replicated below.\n\nevent &lt;- c(\"11-Apr-1964\",\"30-Mar-1964\",\"3-Mar-1971\",\"17-Mar-1975\",\"7-Feb-1998\",\"1-Jan-2016\")\nduration &lt;- c(5,4,4,4.5,4.5,4)\n\nmean_inflow_dur &lt;- round(mean(duration),2)\ngeomean_inflow_dur &lt;- round(exp(mean(log(duration))),2)\n\ncat(\"The mean inflow duration is \",mean_inflow_dur,\"\\nThe geometric mean inflow duration is \", geomean_inflow_dur)\n\nThe mean inflow duration is  4.33 \nThe geometric mean inflow duration is  4.32\n\n\nBoth 4-day and 5-day could be considered for the critical inflow duration. During the project, the 4-day inflow was selected as the critical duration.\nAs a preemptive analysis of seasonallity, the stage seasonallity was analyzed.\n\nggplot(data = annual_max_stage88,aes(x = Month))+\n  geom_histogram(binwidth=1,fill=\"lightblue\",color=\"black\",alpha=0.9)+\n  stat_density(aes(y = ..count..), geom = \"line\", color = \"green3\", size = 1) +\n  scale_x_continuous(breaks = seq(1,12,1),\n                     labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"June\",\"July\",\"Aug\",\"Sept\",\"Oct\",\"Nov\",\"Dec\"))+\n  scale_y_continuous(breaks = seq(0,20,2),minor_breaks = seq(0,20,1))+\n  theme_USACE()+\n  labs(x = \"Month\",y=\"Count\",subtitle = \"Based on AMS of POR Stage Elev.(1962 - 2024)\")+\n  ggtitle(\"Flood Seasonality at Thurmond Dam\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))"
  },
  {
    "objectID": "JST_inR.html#unregulated-4-day-inflow-ams",
    "href": "JST_inR.html#unregulated-4-day-inflow-ams",
    "title": "Thurmond PA - R Scripting",
    "section": "",
    "text": "The next steps were to create a timeseries of unregulated inflow data that can be used to create an AMS of 4-day inflow volumes. This section incorporates code used in CreatingUnregulatedData_Augusta.R and Crtical Volume Duration.R.\nThe Augusta USGS Streamgage was the subject of a 1990 USACE & USGS report that analyzed the relationship between regulated and unregulated flow-frequency estimates. Due to the existence of this study and the longer gage record, the Augusta gage was used as the primary data source for inflow data. The overlapping record of the Augusta gage with the former USGS streamgage at JST Dam was used to verify the drainage area ratio from the design memo (0.93).\n\\[ Q_T = 0.93 * Q_A\\]\nIn order to create a 4-day inflow volume annual maximum series (AMS), the daily streamflow data at the Augusta gage was processed using the following steps:\n\nThe effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\nDaily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in Figure 14 (PA Report - Chapter 4). This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n4-day unregulated volume AMS was obtained for each water year\nThe peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\nThe peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\nThe drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\nThese steps & figures will be replicated below.\n\n\nBefore any of the steps above, the data should be imported and formatted.\n\n# Import Augusta Daily & Format Dates\nlibrary(\"dataRetrieval\")\n\n# Import using Augusta Site Number\nsiteNo &lt;- \"02197000\"\npCode &lt;- \"00060\" #00061 - # Check codes from USGS\nstart.date &lt;- \"1700-01-01\" # arbitraty to get all info\nend.date &lt;- \"2024-12-31\"\n\naugusta_daily &lt;- readNWISdv(siteNumbers = siteNo,\n                            parameterCd = pCode,\n                            startDate = start.date,\n                            endDate = end.date)\n\naugusta_daily &lt;- renameNWISColumns(augusta_daily)\n\naugusta_daily &lt;- augusta_daily %&gt;% mutate(DT = as.Date(Date),.before = Flow)\n\n# Add water year column\naugusta_daily &lt;- augusta_daily %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_daily &lt;- augusta_daily %&gt;% mutate(WaterYear = ifelse(Mon &gt; 9, Yr + 1, Yr))\n\n# Make sure the data is there\nggplot(data = augusta_daily) + geom_line(aes(x=DT,y=Flow)) + theme_bw()\n\n\n\n\n\n\n\n\nThe regulated-unregulated relationship was digitized from the 1990 report and imported into R.\n\n# Import Digitized Data for Figure 35 (1990) - Exceedance Prob for Unregulated and Regulated\nfig35 &lt;- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Figure35data.csv\")\nfig35 &lt;- fig35 %&gt;% mutate(NEP = 100-ExceedanceProb)\n\n# Convert Reg and Unreg at Augusta\naugusta_conversion &lt;- fig35 %&gt;% mutate(Aug_RegQ = PeakQRegulated,Aug_UnregQ = PeakQUnregulated)\naugusta_conversion &lt;- augusta_conversion %&gt;% select(c(Aug_RegQ,Aug_UnregQ))\n\n# Recreate Figure 35\nlogscalebreaks = c(1000,5000,10000,20000,50000,100000,200000,500000,1000000)\nlogscaleminor_breaks = c(seq(2000, 9000, by = 1000),seq(20000, 90000, by = 10000),\n                         seq(200000, 900000, by = 100000))\n\nggplot(data=fig35,aes(x=PeakQUnregulated,y=PeakQRegulated))+\n  geom_point()+\n  geom_line()+\n  theme_USACE()+\n  ggtitle(\"Savannah River at Augusta, GA (02197000)\")+\n  labs(x=\"Unregulated Discharge (cfs)\",y=\"Regulated Discharge (cfs)\",subtitle = \"Relation between regulated peak discharges and unregulated peak discharges\",\n       color = \"Legend\")+\n  theme(plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5),legend.position = \"bottom\")+\n  geom_abline(intercept = 0, slope = 1, color = \"blue1\", linetype = \"dashed\") +\n  scale_y_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  scale_x_log10(labels = label_comma(drop0trailing = TRUE),\n                breaks = logscalebreaks,\n                minor_breaks = logscaleminor_breaks)+\n  coord_cartesian(xlim = c(5000,1000000),ylim = c(5000,1000000))\n\n\n\n\n\n\n\n\nThe Peak Flow dataset at Augusta will also be used. This data was obtained from the USGS website. Additional post processing was done to make this easier (i.e. some manual entry to Flow Notes)\n\n# Import and Format\naugusta_peak_ams &lt;- read.csv(\"E:/1.Thurmond/Chapter 4/HH/Data/Augusta Gage/csv/Augusta_Peak_AMS.csv\",header = T)\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% mutate(DT = as.Date(Date),.before = Flow)\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% mutate(Yr = year(DT),Mon = month(DT),Day = day(DT))\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% mutate(WaterYear = ifelse(Mon &gt; 9, Yr + 1, Yr))\naugusta_peak_ams &lt;- augusta_peak_ams %&gt;% rename(Peak_Flow = Flow)\n\n\nThe effect of regulation was assumed to start in 1951, which was when deliberate impoundment began.\n\n\n# Regulation Assumed to start in 1951\nregulation_wy &lt;- 1951\npre_reg &lt;- augusta_daily %&gt;% filter(WaterYear &lt; regulation_wy)\naug_reg &lt;- augusta_daily %&gt;% filter(WaterYear &gt;= regulation_wy)\n\npre_reg_peak &lt;- augusta_peak_ams %&gt;% filter(WaterYear &lt; regulation_wy)\naug_reg_peak &lt;- augusta_peak_ams %&gt;% filter(WaterYear &gt;= regulation_wy)\n\n\nDaily regulated streamflow data at Augusta was converted to unregulated daily streamflow using the relationship shown in the relationship above. This figure was digitized by hand from the 1990 study. Unregulated flows were interpolated from the curve shown in the figure.\n\n\n# Estimate Unregulated from approx function\nAugusta_Unreg &lt;- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ, xout = aug_reg$Flow)[2]\nAugusta_Unreg_peak &lt;- approx(augusta_conversion$Aug_RegQ, augusta_conversion$Aug_UnregQ,\n                             xout=aug_reg_peak$Peak_Flow)[2]\n\n# add to regulated data\naug_reg &lt;- aug_reg %&gt;% mutate(Unregulated_Flow = round(Augusta_Unreg$y,0))\naug_reg_peak &lt;- aug_reg_peak %&gt;% mutate(Unregulated_PeakFlow = round(Augusta_Unreg_peak$y,0))\n\n# create a 1:1 column of unregulated flow to make bind_rows easier\npre_reg &lt;- pre_reg %&gt;% mutate(Unregulated_Flow = round(Flow,0))\npre_reg_peak &lt;- pre_reg_peak %&gt;% mutate(Unregulated_PeakFlow = round(Peak_Flow,0))\n\n# Combine datasets\naugusta_daily_unreg &lt;- bind_rows(pre_reg,aug_reg)\nAMS_Peak &lt;- bind_rows(pre_reg_peak,aug_reg_peak)\n\n# remove any confusion\nrm(Augusta_Unreg)\nrm(Augusta_Unreg_peak)\n\n# Check results for sanity \nggplot(data = augusta_daily_unreg) +\n  geom_line(aes(x=DT,y=Unregulated_Flow,color = \"Unregulated\"))+\n  geom_line(aes(x=DT,y=Flow,color=\"Existing\"))+\n  theme_USACE()+\n  labs(x = \"Date\",\n       y = \"Daily Flow (cfs)\",\n       title = \"Daily Flow Values Augusta (02197000)\",\n       subtitle = \"Post-WY 1951 Converted to Unregulated\")+\n  scale_y_continuous(breaks = seq(0,250000,50000),minor_breaks = seq(0,250000,10000))+\n  scale_x_date(breaks = \"10 year\",minor_breaks = \"1 year\")+\n  scale_color_manual(values = c(\"#377EB8\", \"red2\"), \n                     name = \"Legend\", \n                     labels = c(\"Existing\",\"Unregulated\"))+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),legend.position = c(.85,.8))\n\n\n\n\n\n\n\n\n\n4-day unregulated volumes were estimated from available daily unregulated streamflow data, now spanning 1883 – present (with some gaps)\n\n\n# Create Data with gaps to avoid rolling mean over the gaps (i.e. 1891-12-31 & 1896-01-01)\ncomplete_dates &lt;- seq(min(augusta_daily_unreg$DT), max(augusta_daily_unreg$Date), by = \"day\")\naug_unreg_complete &lt;- merge(augusta_daily_unreg, data.frame(Date = complete_dates), by = \"Date\", all = TRUE)\n# aug_unreg_complete[3010:3020,]  # check\n#naniar::vis_miss(aug_unreg_complete) # see gaps in data, uncomment to use\n\ncritdur &lt;- 4\n# rollmean is a 4 day rolling average. rollmeanr indicates that the window is aligned to the \"right\"\n# This will compute a time series of 4-day inflow volumes\naug_unreg_complete &lt;- aug_unreg_complete %&gt;% mutate(VolDur_4day = round(rollmeanr(Unregulated_Flow,k = critdur, fill=NA),0))\n\n\n4-day unregulated volume AMS was obtained for each water year\n\n\nAMS_4day &lt;- aug_unreg_complete %&gt;%\n  # Group by WY\n  group_by(WaterYear) %&gt;%\n  # Take the largest value from each WY\n  top_n(1, VolDur_4day) %&gt;%\n  # Ensures no duplicate years\n  distinct(WaterYear, .keep_all = TRUE) %&gt;%\n  ungroup()\n\n# Combine Peak and 4-day AMS to help stay organized\nall_AMS &lt;- left_join(AMS_Peak,AMS_4day,by = \"WaterYear\")\ncolnames(all_AMS)\n\n [1] \"Agency\"               \"Site\"                 \"Date.x\"              \n [4] \"DT.x\"                 \"Peak_Flow\"            \"Flow_notes\"          \n [7] \"Gage\"                 \"Gage_notes\"           \"Yr.x\"                \n[10] \"Mon.x\"                \"Day.x\"                \"WaterYear\"           \n[13] \"Unregulated_PeakFlow\" \"Date.y\"               \"agency_cd\"           \n[16] \"site_no\"              \"DT.y\"                 \"Flow\"                \n[19] \"Flow_cd\"              \"Yr.y\"                 \"Mon.y\"               \n[22] \"Day.y\"                \"Unregulated_Flow\"     \"VolDur_4day\"         \n\nall_AMS &lt;- all_AMS %&gt;% select(c(DT.x,Peak_Flow,Flow_notes,WaterYear,Unregulated_PeakFlow,DT.y,Unregulated_Flow,VolDur_4day))\nall_AMS &lt;- all_AMS %&gt;% rename(DT_Peak = DT.x, DT_Daily = DT.y)\n\n\nThe peak-to-volume ratio was calculated using the Augusta gage peak-flow AMS. The peak flow was divided by the 4-day volume for corresponding floods. The average peak-to-volume ratio was 1.54.\n\n\n# Find the peaks and 4-day vols that are from the same event. 4 is the search threshold then\nmatchingAMS &lt;- all_AMS %&gt;% filter(DT_Peak &gt;= DT_Daily - 4 | DT_Peak &lt;= DT_Daily + 4)\nmatchingAMS &lt;- matchingAMS %&gt;% mutate(PeaktoVol = Unregulated_PeakFlow/VolDur_4day)\n\n# Now Add this peak to vol data back to the AMS dataset\nall_AMS &lt;- all_AMS %&gt;% mutate(Ratio_Peak_4d = ifelse(DT_Peak &gt;= DT_Daily - 4 | DT_Peak &lt;= DT_Daily + 4,Unregulated_PeakFlow/VolDur_4day,NA))\n\ncat(\"The mean peak-to-volume ratio is \",round(mean(matchingAMS$PeaktoVol),2),\"\\nThe geometric mean peak-to-volume ratio is \", round(exp(mean(log(matchingAMS$PeaktoVol))),2),\"\\nThe median peak-to-volume ratio is \",round(median(matchingAMS$PeaktoVol),2))\n\nThe mean peak-to-volume ratio is  1.54 \nThe geometric mean peak-to-volume ratio is  1.46 \nThe median peak-to-volume ratio is  1.39\n\n\n\nggplot(data = matchingAMS)+\n  geom_histogram(aes(x =PeaktoVol),fill=\"lightblue\",color=\"black\",alpha=0.9,\n                 breaks = seq(0.9,3.0,0.1))+\n  theme_USACE()+\n  ggtitle(\"Ratio of Peak Flow to 4-Day Volume at Augusta (02197000)\")+\n  labs(x=\"Peak Flow/4-Day Vol\", y=\"Count\")+\n  scale_x_continuous(breaks = seq(0.9,3.0,0.1),minor_breaks = seq(0,3.0,0.05))+\n  scale_y_continuous(breaks = seq(0,25,1))+\n  geom_vline(xintercept = mean(matchingAMS$PeaktoVol), color=\"orange2\")+\n  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+\n  theme(axis.text.x = element_text(size=6.5))\n\n\n\n\n\n\n\n\n\nThe peak-to-volume ratio was used to estimate the 4-day volume annual maximum values for the gaps in the daily data (1892 to 1896 and 1906 to 1925).\n\n\n# Peak to volume ratio\nPtVratio_4day &lt;- mean(matchingAMS$PeaktoVol)\n\n# Calculate missing 4 day volumes\nall_AMS &lt;- all_AMS %&gt;% mutate(Peak_to_4day = round(Unregulated_PeakFlow/PtVratio_4day,0))\n\nall_AMS &lt;- all_AMS %&gt;% mutate(Complete_4Day_AMS = ifelse(is.na(all_AMS$VolDur_4day),Peak_to_4day,VolDur_4day))\n\n\nThe drainage area ratio (0.93) was applied to the 4-day volume AMS (regardless of year) to estimate the 4-day inflow volume AMS at Thurmond Dam.\n\n\n# Previously calculated drainage area ratio\nDAratio &lt;- 0.93\n\n# Multiply the Augusta AMS values by the DA ratio\nall_AMS &lt;- all_AMS %&gt;% mutate(Thurm_AMS_4day = round(DAratio*Complete_4Day_AMS,0))\n\nBONUS STEP: Prepare the data to be used in BestFit. This will use the flow notes to set perception threshold values & interval values (assumed to be +/- 20%). The interval calculation can be tough to decipher via code. It has been written out as an equation below.\n\\[ Interval_L = 0.80 * 0.93 * PeakQ/PeakToVol\\] \\[ Interval_U = 1.20 * 0.93 * PeakQ/PeakToVol\\]\n\n# Rename some columns to make it easier - this is because my variable naming was not as intuitive as it should have been\nall_AMS &lt;- all_AMS %&gt;% rename(Aug_AMS_1day = Unregulated_Flow, \n                              Aug_AMS_4day = VolDur_4day,\n                              Aug_Complete_4Day_AMS = Complete_4Day_AMS)\n\n# If there is no peak to 4day ratio, this was historic peak flow information\nall_AMS &lt;- all_AMS %&gt;% mutate(BestFitNotes = ifelse(is.na(Ratio_Peak_4d),\"Estimated from Peak AMS\",NA))\n\n# Computing intervals. ifelse ensures it only computs missing ones\nall_AMS &lt;- all_AMS %&gt;% mutate(Interval_Lower =\n                                ifelse(WaterYear&lt;1876,round((DAratio*0.8*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_MostLikely =                                 \n                                ifelse(WaterYear&lt;1876,round((DAratio*1.0*Unregulated_PeakFlow)/PtVratio_4day,0),NA),\n                              Interval_Upper =\n                                ifelse(WaterYear&lt;1876,round((DAratio*1.2*Unregulated_PeakFlow)/PtVratio_4day,0),NA))\n\nThe allAMS dataframe can be exported to .csv or it can be further filtered to only include fields required for BestFit.\n\n# Exporting allAMS - commented out\n# write.csv(all_AMS,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_AMS_Peak_1day_4day.csv\",row.names = F)\n\n# Selecting fields from allAMS that are required for BestFit\nbestfit_input &lt;- all_AMS %&gt;% select(c(WaterYear,Peak_Flow,Flow_notes,Unregulated_PeakFlow,\n                                      Thurm_AMS_4day,BestFitNotes,\n                                      Interval_Lower,Interval_MostLikely,Interval_Upper))\n\n# Export BestFit to csv - commented out\n#write.csv(bestfit_input,\"E:/1.Thurmond/Chapter 4/HH/Data/Savannah River - USGS Unregulated Study/Thurmond_Unreg_BestFitINPUT.csv\",row.names = F)"
  },
  {
    "objectID": "JST_inR.html#theme-usace",
    "href": "JST_inR.html#theme-usace",
    "title": "Thurmond PA - R Scripting",
    "section": "",
    "text": "Move this before the other code blocks for ggplot themes. If issues continue, remove +theme_USACE() from the ggplot chunks\n\ntheme_USACE &lt;-  function(base_size = 8){theme(\n  text = element_text(family = 'serif', color = 'black'),\n  line = element_line(colour = 'black', linewidth = 0.2), \n  rect = element_rect(colour = 'black', linewidth = 0.2),\n  plot.title = element_text(vjust = 3, size = 9),\n  plot.margin = unit(c(1,1,1,1), 'lines'),\n  panel.border = element_rect(fill = F),\n  panel.grid.major = element_line(colour = 'grey50', linewidth = 0.2),\n  panel.grid.minor = element_line(colour = 'grey75', linewidth = 0.1),\n  panel.background = element_rect(fill = 'white'),\n  #defaults legend to upper left, can/should be overridden based on graph\n  #legend.background = element_blank(),\n  legend.background = element_rect(fill = \"lightgrey\", colour = \"black\"),\n  legend.justification = c(\"left\", \"top\"),\n  legend.position = c(0.8, 0.5),\n  # this value should be adjusted dependent on \n  # graph with the addition of another \n  # theme(legend.position = c(X, Y)) argument after theme_USACE()\n  # or... for no legend \n  # legend.position = element_blank(),\n  legend.key = element_blank(),\n  legend.title = element_text(size = 9),\n  #legend.title = element_blank(), \n  axis.title.x = element_text(size = 9),\n  axis.title.y = element_text(angle = 90, size = 9),\n  axis.text.x = element_text(margin = margin(8, 0, 0, 0)),\n  axis.text.y = element_text(margin = margin(0, 8, 0, 0)),\n  axis.ticks.length = unit(0.25 , 'cm')\n)}"
  }
]